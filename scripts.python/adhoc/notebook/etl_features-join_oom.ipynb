{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETLFeatures OOM problem\n",
    "\n",
    "Проекты джойнилки с количеством джойнов более 20 периодически падают с OOM на стадии джойна, перед записью результата в таблицу\n",
    "\n",
    "Шаг 1: воспроизвести проблему, джойнить 30 датафреймов (доменов) и смотреть на метрики/логи в spark UI\n",
    "\n",
    "Шаг 2: эксперементировать с решениями проблемы\n",
    "\n",
    "После добавления чекпойнта на каждые 10 джойнов, падать на spill перестало, начало падать `Container killed by YARN for exceeding memory limits`\n",
    "\n",
    "Увеличение памяти на экзекуторах и повышение количества партиций позволило успешно завершить джобу без единого task fail (если не считать container preemtion).\n",
    "\n",
    "Использование `df.checkpoint(eager=true)` вполне работает, но генерирует раза в 3 больше нагрузку на hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from pprint import pformat\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from operator import and_\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "from pyspark.sql.types import MapType, ArrayType, FloatType, StringType, NumericType\n",
    "\n",
    "if six.PY3:\n",
    "    from functools import reduce  # make flake8 happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(dict(os.environ), width=1)\n",
    "def log(obj, msg=\"\"):\n",
    "    if msg: print(msg)\n",
    "    print(\"type: {}\\ndata: {}\".format(type(obj), pformat(obj, indent=1, width=1)))\n",
    "\n",
    "log(os.environ, \"os.environ\")\n",
    "print()\n",
    "log(dict(os.environ), \"dict(os.environ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Pack executable prj conda environment into zip\n",
    "TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "env_name = os.path.basename(env_dir)\n",
    "env_archive = \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)\n",
    "\n",
    "# you need this only first time!\n",
    "# !rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n",
    "\n",
    "# b.config(\"spark.yarn.dist.archives\", env_archive)\n",
    "log(env_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with prj conda environment and JVM extensions\n",
    "# `spark-submit ... --driver-java-options \"-Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\"`\n",
    "# spark.driver.extraJavaOptions\n",
    "\n",
    "queue = \"root.priority\"\n",
    "\n",
    "# \"spark.driver.extraJavaOptions\", \"-Xss10M\"\n",
    "# catalyst SO while building parts. filter expression\n",
    "\n",
    "# 200 GB of data\n",
    "sssp = (200 * 4) * 2 * 2 * 4\n",
    "\n",
    "spark = (\n",
    "SparkSession.builder\n",
    "    .master(\"yarn-client\")\n",
    "    .appName(\"TRG-77961-test-ipynb\")\n",
    "    .config(\"spark.yarn.queue\", queue)\n",
    "    .config(\"spark.sql.shuffle.partitions\", sssp)\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"24G\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"8G\")\n",
    "    .config(\"spark.driver.memory\", \"4G\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/home/vlk/driver2_log4j.properties\")\n",
    "    .config(\"spark.speculation\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"512\")\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\n",
    "    .config(\"spark.network.timeout\", \"800s\")\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition\", \"true\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions\", \"1000000\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions.pernode\", \"100000\")\n",
    "    .config(\"spark.hadoop.hive.metastore.client.socket.timeout\", \"60s\")\n",
    "    .config(\"spark.ui.enabled\", \"true\")\n",
    "    .config(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "    .config(\"spark.yarn.dist.archives\", env_archive)\n",
    "    .getOrCreate()\n",
    ")\n",
    "# .config(\"spark.driver.extraJavaOptions\", \"-Xss10M -Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\")\n",
    "# .config(\"spark.jars\", \"hdfs:/lib/dm/prj-transformers-assembly-dev-1.5.1.jar\")\n",
    "\n",
    "sql_ctx = SQLContext(spark.sparkContext)\n",
    "(spark, sql_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of env. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pformat\n",
    "\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import DataFrame, SQLContext\n",
    "from pyspark.sql.types import (\n",
    "    MapType, ArrayType, FloatType, DoubleType, StringType, StructType, IntegralType, IntegerType\n",
    ")\n",
    "from pyspark.sql.utils import CapturedException\n",
    "from pyspark.ml.wrapper import JavaWrapper\n",
    "\n",
    "from luigi.contrib.hdfs import HdfsTarget\n",
    "\n",
    "from dmprj.apps.utils.common import add_days\n",
    "from dmprj.apps.utils.common.hive import format_table, select_clause\n",
    "from dmprj.apps.utils.control.client.exception import FailedStatusException, MissingDepsStatusException\n",
    "from dmprj.apps.utils.control.client.logs import ControlLoggingMixin\n",
    "from dmprj.apps.utils.common.external_program import AvoidLuigiFlatTaskRunner\n",
    "\n",
    "from dmprj.apps.utils.common import unfreeze_json_param\n",
    "from dmprj.apps.utils.common.fs import HdfsClient\n",
    "from dmprj.apps.utils.common.hive import FindPartitionsEngine\n",
    "from dmprj.apps.utils.common.luigix import HiveTableSchemaTarget\n",
    "\n",
    "from dmprj.apps.utils.common.hive import select_clause\n",
    "from dmprj.apps.utils.common.spark import CustomUDFLibrary, insert_into_hive\n",
    "from dmprj.apps.utils.common.luigix import HiveExternalTask, HiveGenericTarget\n",
    "from dmprj.apps.utils.control.luigix import ControlApp, ControlDynamicOutputPySparkTask\n",
    "\n",
    "from dmprj.common.hive import HiveMetastoreClient, HiveThriftSASLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomUDFLibrary(spark, \"hdfs:/lib/dm/prj-transformers-assembly-dev-1.5.1.jar\").register_all_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, message=\"dataframe\", nlines=20, truncate=False, heavy=True):\n",
    "    if not heavy: print(\"\\n{}, rows: {}:\".format(message, df.count()))\n",
    "    df.printSchema()\n",
    "    if not heavy: df.show(nlines, truncate)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"dmprj_source\"\n",
    "table = \"hid_dataset_3_0\"\n",
    "dt = \"2022-03-30\"\n",
    "uid_type = \"HID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_expr = \"select * from {db}.{table} where dt='{dt}' and uid_type='{uid_type}'\".format(**globals())\n",
    "log(source_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = spark.sql(source_expr).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(source_df, \"source\", heavy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 domains\n",
    "columns_text = \"\"\"\n",
    " |-- uid: string (nullable = true)\n",
    " |-- shows_activity: array (nullable = true)\n",
    " |-- banner_cats_ctr_wscore_30d: map (nullable = true)\n",
    " |-- hw_osm_ratios: map (nullable = true)\n",
    " |-- hw_osm_stats: array (nullable = true)\n",
    " |-- topics_motor200: map (nullable = true)\n",
    " |-- living_region_ids: map (nullable = true)\n",
    " |-- living_region_stats: array (nullable = true)\n",
    " |-- all_profs: array (nullable = true)\n",
    " |-- sn_epf: array (nullable = true)\n",
    " |-- sn_topics100: array (nullable = true)\n",
    " |-- app_stats: array (nullable = true)\n",
    " |-- app_topics100: map (nullable = true)\n",
    " |-- tp_os_counts: map (nullable = true)\n",
    " |-- tp_device_stats: array (nullable = true)\n",
    " |-- app_cats_activity: map (nullable = true)\n",
    " |-- mob_operators: map (nullable = true)\n",
    " |-- device_vendors: map (nullable = true)\n",
    " |-- app_events: map (nullable = true)\n",
    " |-- onelink_payments: map (nullable = true)\n",
    " |-- onelink_logins: map (nullable = true)\n",
    " |-- onelink_login_recencies: map (nullable = true)\n",
    " |-- topgoal_topics200: map (nullable = true)\n",
    " |-- mpop_senders: map (nullable = true)\n",
    " |-- app_cats_pc_cri_wscore_180d: map (nullable = true)\n",
    " |-- app_cats_installed: map (nullable = true)\n",
    " |-- dt: string (nullable = true)\n",
    " |-- uid_type: string (nullable = true)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <type 'list'>\n",
      "data: ['shows_activity',\n",
      " 'banner_cats_ctr_wscore_30d',\n",
      " 'hw_osm_ratios',\n",
      " 'hw_osm_stats',\n",
      " 'topics_motor200',\n",
      " 'living_region_ids',\n",
      " 'living_region_stats',\n",
      " 'all_profs',\n",
      " 'sn_epf',\n",
      " 'sn_topics100',\n",
      " 'app_stats',\n",
      " 'app_topics100',\n",
      " 'tp_os_counts',\n",
      " 'tp_device_stats',\n",
      " 'app_cats_activity',\n",
      " 'mob_operators',\n",
      " 'device_vendors',\n",
      " 'app_events',\n",
      " 'onelink_payments',\n",
      " 'onelink_logins',\n",
      " 'onelink_login_recencies',\n",
      " 'topgoal_topics200',\n",
      " 'mpop_senders',\n",
      " 'app_cats_pc_cri_wscore_180d',\n",
      " 'app_cats_installed']\n"
     ]
    }
   ],
   "source": [
    "def strip(x):\n",
    "    x = x.replace(\"|--\", \"\").strip()\n",
    "    for non_domain_prefix in [\"uid\", \"dt\", \"uid_type\"]:\n",
    "        if x.startswith(non_domain_prefix):\n",
    "            return \"\"\n",
    "    return x.split(\":\")[0]\n",
    "\n",
    "domains_names = [strip(x) for x in columns_text.split(\"\\n\") if strip(x)]\n",
    "log(domains_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_num = 30\n",
    "\n",
    "def domain_name(i):\n",
    "    return domains_names[i % len(domains_names)]\n",
    "\n",
    "domains = [\n",
    "    source_df.selectExpr(\"uid\", \"uid_type\", \"{} as d{}\".format(domain_name(i), i+1))\n",
    "    for i in range(domains_num)\n",
    "]\n",
    "\n",
    "log(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "data: DataFrame[uid: string, uid_type: string, d1: array<float>, d2: map<string,float>, d3: map<string,float>, d4: array<float>, d5: map<string,float>, d6: map<string,float>, d7: array<float>, d8: array<float>, d9: array<float>, d10: array<float>, d11: array<float>, d12: map<string,float>, d13: map<string,float>, d14: array<float>, d15: map<string,float>, d16: map<string,float>, d17: map<string,float>, d18: map<string,float>, d19: map<string,float>, d20: map<string,float>, d21: map<string,float>, d22: map<string,float>, d23: map<string,float>, d24: map<string,float>, d25: map<string,float>, d26: array<float>, d27: map<string,float>, d28: map<string,float>, d29: array<float>, d30: map<string,float>]\n"
     ]
    }
   ],
   "source": [
    "keys = [\"uid\", \"uid_type\"]\n",
    "join = \"left_outer\"\n",
    "left = domains[0]\n",
    "\n",
    "# OOM here, need another solution\n",
    "def join_domains(left_df, keys, join, domains):\n",
    "    for right_df in domains:\n",
    "        left_df = left_df.join(right_df, keys, join)\n",
    "    return left_df\n",
    "\n",
    "# checkpoint each 10 step\n",
    "hdfs_tmp_dir = \"hdfs:/user/vlk/tmp/TRG-77961-etl_features-OOM\"\n",
    "spark.sparkContext.setCheckpointDir(os.path.join(hdfs_tmp_dir, \"sc_checkpoint\"))\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "\n",
    "def checkpoint(df, step):\n",
    "    def _checkpoint():\n",
    "        return df.checkpoint(eager=True)\n",
    "        # return df.checkpoint(eager=False)  # job fail almost immediately after join stage started, with \n",
    "    # SparkOutOfMemoryError: Unable to acquire 68 bytes of memory, got 0\n",
    "\n",
    "    def _checkpoint_manual():\n",
    "        # works fine, but slow\n",
    "        checkpoint_dir = os.path.join(hdfs_tmp_dir, \"checkpoint_step{}\".format(step))\n",
    "        _ = (\n",
    "            df\n",
    "            .write\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"compression\", \"gzip\")\n",
    "            .option(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "            .parquet(checkpoint_dir)\n",
    "        )\n",
    "        return spark.read.parquet(checkpoint_dir)\n",
    "\n",
    "    if step > 0 and step % CHECKPOINT_INTERVAL == 0:\n",
    "        return _checkpoint()\n",
    "    return df\n",
    "\n",
    "def join_domains_using_checkpoints(left_df, keys, join, domains):\n",
    "    for step, right_df in enumerate(domains, 1):\n",
    "        left_df = checkpoint(left_df.join(right_df, keys, join), step)\n",
    "    return left_df\n",
    "\n",
    "joined_df = join_domains(left, keys, join, domains[1:])\n",
    "# joined_df = join_domains_using_checkpoints(left, keys, join, domains[1:])\n",
    "\n",
    "log(joined_df)\n",
    "# https://rm.adh.vk.team/proxy/application_1649019708481_15899/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(joined_df, \"joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = os.path.join(hdfs_tmp_dir, \"result\")\n",
    "\n",
    "_ = (\n",
    "    joined_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"gzip\")\n",
    "    .option(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "    .parquet(result_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check luigi tasks pool runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmprj.apps.utils.common.external_program import BusyWaitPoolRunner\n",
    "from pathos.multiprocessing import ProcessPool\n",
    "\n",
    "class AvoidLuigiFlatTaskRunner(ControlLoggingMixin):\n",
    "    \"\"\"Run a batch of luigi tasks in parallel without using native luigi worker-scheduler interface.\n",
    "\n",
    "    The main reason behind this is avoiding strange deadlocks within luigi framework, when spawning more tasks within a\n",
    "    task run method. The only limitation here is that all input tasks for this kind of runner should have only external\n",
    "    dependencies.\n",
    "\n",
    "    :param tasks: list of luigi tasks to run.\n",
    "    :param processes: size of processing pool or None (by default) for using len(tasks) as the pool size.\n",
    "    :param log_url: Control logging endpoint.\n",
    "    :param raise_on_task_failure: if True, any task that fails aborts all pool processes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tasks, processes=None, log_url=None, raise_on_task_failure=True):\n",
    "        self.tasks = tasks\n",
    "        self.processes = len(tasks) if processes is None else processes\n",
    "        self.log_url = log_url\n",
    "        self.raise_on_task_failure = raise_on_task_failure\n",
    "        self._validate_tasks()\n",
    "\n",
    "    def _validate_tasks(self):\n",
    "        for task in self.tasks:\n",
    "            if not isinstance(task, luigi.Task):\n",
    "                raise TypeError(\"All tasks must be `luigi.Task`, got {}\".format(task))\n",
    "            for required_task in task.deps():\n",
    "                if not isinstance(required_task, luigi.ExternalTask):\n",
    "                    raise TypeError(\"Got non-external dependency {} for a task {}\".format(required_task, task))\n",
    "                if not required_task.complete():\n",
    "                    raise ValueError(\"Incomplete external dependency {} for a task {}\".format(required_task, task))\n",
    "\n",
    "    def run(self, log_traceback=True):\n",
    "        return BusyWaitPoolRunner(\n",
    "            pool=ProcessPool(nodes=self.processes),\n",
    "            log_traceback=log_traceback,\n",
    "            log_url=self.log_url\n",
    "        ).run(\n",
    "            self._run_task,\n",
    "            self.tasks\n",
    "        )\n",
    "\n",
    "    def _run_task(self, task):\n",
    "        if not task.complete():\n",
    "            self.info(\"Running a task {}\".format(task))\n",
    "            try:\n",
    "                task.run()\n",
    "            except Exception as e:\n",
    "                task.on_failure(e)\n",
    "                success_flag = False\n",
    "                if self.raise_on_task_failure:\n",
    "                    raise e\n",
    "            else:\n",
    "                task.on_success()\n",
    "                success_flag = True\n",
    "        else:\n",
    "            self.info(\"Task {} is already complete\".format(task))\n",
    "            success_flag = True\n",
    "\n",
    "        return task.task_id, success_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T(luigi.Task):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(T, self).__init__()\n",
    "    def complete(self):\n",
    "        return False\n",
    "    def run(self):\n",
    "        raise ValueError(\"oops\")\n",
    "    def on_failure(self, exception):\n",
    "        log(exception, msg=\"\\n on_failure\")\n",
    "        raise ValueError(\"failed on_failure\")\n",
    "        return \"failure: {}\".format(exception)\n",
    "    def on_success(self):\n",
    "        pass\n",
    "\n",
    "tasks = [T()]\n",
    "\n",
    "results = None\n",
    "try:\n",
    "    results = AvoidLuigiFlatTaskRunner(\n",
    "        tasks=tasks,\n",
    "        raise_on_task_failure=False,\n",
    "    ).run(\n",
    "        log_traceback=True\n",
    "    )\n",
    "except BaseException as e:\n",
    "        log(e, msg=\"\\n boo-hoo! Exception:\")\n",
    "\n",
    "log(results, msg=\"\\n results:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
