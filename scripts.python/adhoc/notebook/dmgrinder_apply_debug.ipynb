{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hid_dataset_3_0 invalid records\n",
    "\n",
    "Шаг apply начал падать (в декабре 2022) с сообщениями типа\n",
    "```s\n",
    "\n",
    "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
    "...\n",
    "  File \"/prj/adapters/apply.py\", line 269, in _transform_rows\n",
    "  File \"/prj/adapters/model/base.py\", line 313, in apply\n",
    "    columns = self.predict_adapter.predict(self.estimator, dataset)\n",
    "  File \"/prj/adapters/predict.py\", line 74, in predict\n",
    "    predicts = dict(predict_method(dataset.x, *steps, **predict_params))\n",
    "  File \"/prjcore/predictors/pipeline.py\", line 215, in yield_from_steps\n",
    "    Xt = step.transform(Xt, **trf_params)\n",
    "  File \"/prjcore/transformers/preprocessing.py\", line 207, in transform\n",
    "    return su.check_array(X_res, accept_sparse=sparse_format)\n",
    "  File \"/sklearn/utils/validation.py\", line 517, in check_array\n",
    "    accept_large_sparse=accept_large_sparse)\n",
    "  File \"/sklearn/utils/validation.py\", line 350, in _ensure_sparse_format\n",
    "    allow_nan=force_all_finite == 'allow-nan')\n",
    "  File \"/sklearn/utils/validation.py\", line 56, in _assert_all_finite\n",
    "    raise ValueError(msg_err.format(type_err, X.dtype))\n",
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
    "\n",
    "```\n",
    "\n",
    "Метом джойна left_anti витрины с размеченной выборкой получил 100 записей, сохранил их в паркет\n",
    "`hdfs:/user/vlk/TRG-80367-joiner_features/hid_dataset_3_0/2022-12-03/skip-clal_adhoc_audience-apply`\n",
    "\n",
    "Теперь надо провести анализ этой вырезки и понять, что не так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from pprint import pformat\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from operator import and_\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "import luigi\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "from pyspark.sql.types import MapType, ArrayType, FloatType, StringType, NumericType\n",
    "\n",
    "if six.PY3:\n",
    "    from functools import reduce  # make flake8 happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(dict(os.environ), width=1)\n",
    "def log(obj, msg=\"\"):\n",
    "    if msg: print(msg)\n",
    "    print(\"type: {}\\ndata: {}\".format(type(obj), pformat(obj, indent=1, width=1)))\n",
    "\n",
    "log(os.environ, \"os.environ\")\n",
    "print()\n",
    "log(dict(os.environ), \"dict(os.environ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Pack executable prj conda environment into zip\n",
    "TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "env_name = os.path.basename(env_dir)\n",
    "env_archive = \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)\n",
    "\n",
    "# you need this only first time!\n",
    "# !rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n",
    "\n",
    "# b.config(\"spark.yarn.dist.archives\", env_archive)\n",
    "log(env_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with prj conda environment and JVM extensions\n",
    "# `spark-submit ... --driver-java-options \"-Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\"`\n",
    "# spark.driver.extraJavaOptions\n",
    "\n",
    "queue = \"root.priority\"\n",
    "\n",
    "# \"spark.driver.extraJavaOptions\", \"-Xss10M\"\n",
    "# catalyst SO while building parts. filter expression\n",
    "\n",
    "# 200 GB of data\n",
    "# sssp = (200 * 4) * 2 * 2 * 4\n",
    "sssp = 12\n",
    "\n",
    "spark = (\n",
    "SparkSession.builder\n",
    "    .master(\"yarn-client\")\n",
    "    .appName(\"TRG-92508-test-ipynb\")\n",
    "    .config(\"spark.yarn.queue\", queue)\n",
    "    .config(\"spark.sql.shuffle.partitions\", sssp)\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"6\")\n",
    "    .config(\"spark.executor.memory\", \"6G\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"6G\")\n",
    "    .config(\"spark.driver.memory\", \"4G\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/home/vlk/driver2_log4j.properties\")\n",
    "    .config(\"spark.speculation\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"256\")\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\n",
    "    .config(\"spark.network.timeout\", \"800s\")\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition\", \"true\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions\", \"1000000\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions.pernode\", \"100000\")\n",
    "    .config(\"spark.hadoop.hive.metastore.client.socket.timeout\", \"60s\")\n",
    "    .config(\"spark.ui.enabled\", \"true\")\n",
    "    .config(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "    .config(\"spark.yarn.dist.archives\", env_archive)\n",
    "    .getOrCreate()\n",
    ")\n",
    "# .config(\"spark.driver.extraJavaOptions\", \"-Xss10M -Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\")\n",
    "# .config(\"spark.jars\", \"hdfs:/lib/dm/prj-transformers-assembly-dev-1.5.1.jar\")\n",
    "\n",
    "sql_ctx = SQLContext(spark.sparkContext)\n",
    "(spark, sql_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of env. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn.utils as su\n",
    "\n",
    "from boltons.iterutils import chunked_iter\n",
    "\n",
    "from pprint import pformat\n",
    "\n",
    "import luigi\n",
    "\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql import DataFrame, SQLContext\n",
    "from pyspark.sql.types import (\n",
    "    MapType, ArrayType, FloatType, DoubleType, StringType, StructType, IntegralType, IntegerType\n",
    ")\n",
    "from pyspark.sql.utils import CapturedException\n",
    "from pyspark.ml.wrapper import JavaWrapper\n",
    "\n",
    "from luigi.contrib.hdfs import HdfsTarget\n",
    "\n",
    "from prj.apps.utils.common import add_days\n",
    "from prj.apps.utils.common.hive import format_table, select_clause\n",
    "from prj.apps.utils.control.client.logs import ControlLoggingMixin\n",
    "from prj.apps.utils.common.external_program import AvoidLuigiFlatTaskRunner\n",
    "\n",
    "from prj.apps.utils.common import unfreeze_json_param\n",
    "from prj.apps.utils.common.fs import HdfsClient\n",
    "from prj.apps.utils.common.hive import FindPartitionsEngine\n",
    "from prj.apps.utils.common.luigix import HiveTableSchemaTarget\n",
    "\n",
    "from prj.apps.utils.common.hive import select_clause\n",
    "from prj.apps.utils.common.spark import CustomUDFLibrary, insert_into_hive\n",
    "from prj.apps.utils.common.luigix import HiveExternalTask, HiveGenericTarget\n",
    "from prj.apps.utils.control.luigix import ControlApp, ControlDynamicOutputPySparkTask\n",
    "\n",
    "from prj.common.hive import HiveMetastoreClient, HiveThriftSASLContext\n",
    "\n",
    "from prj.apps.utils.common.spark import PySparkRows2LearningData\n",
    "from prj.adapters.datafile import (\n",
    "    LabelNamesAdapter, LearningDataAdapter, GroupedFeaturesAdapter, AbstractDataFileAdapter\n",
    ")\n",
    "from prj.adapters.dataset import LearnsetAdapter, AbstractDatasetAdapter\n",
    "from prj.adapters.model.oclf import OrdClfLGBMRegressor\n",
    "from prjcore.utils.io import LearningData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomUDFLibrary(spark, \"hdfs:/lib/dm/prj-transformers-assembly-dev-1.5.1.jar\").register_all_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, message=\"dataframe\", nlines=20, truncate=False, heavy=True):\n",
    "    if not heavy: print(\"\\n{}, rows: {}:\".format(message, df.count()))\n",
    "    df.printSchema()\n",
    "    if not heavy: df.show(nlines, truncate)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_dataset_100_path = (\n",
    "    \"hdfs:/user/vlk/TRG-80367-joiner_features/\"\n",
    "    \"hid_dataset_3_0/2022-12-03/skip-clal_adhoc_audience-apply\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = spark.read.parquet(hid_dataset_100_path).persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(features_df, heavy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_rows = features_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(features_rows[44])  # list[Row]\n",
    "# uid=u'7759301908035527299'\n",
    "# tp_device_stats=[3.0750763416290283, 0.0, inf]\n",
    "\n",
    "# found: invalid value: inf in array of float values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = features_rows[44]\n",
    "# log(orig.asDict())\n",
    "tp_device_stats = orig[\"tp_device_stats\"]\n",
    "log(tp_device_stats, \"bad\")\n",
    "tp_device_stats = tp_device_stats[:2] + [0.0]\n",
    "log(tp_device_stats, \"fixed\")\n",
    "\n",
    "fixed = orig.asDict()\n",
    "fixed[\"tp_device_stats\"] = tp_device_stats\n",
    "fixedRow = Row(**fixed)\n",
    "log(fixedRow, \"fixed row\")\n",
    "\n",
    "features_rows[44] = fixedRow\n",
    "log(features_rows[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = rows_transformer.transform(chunk).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"\n",
    "rm -v ./grouped_features.json\n",
    "hdfs dfs -copyToLocal /user/vlk/tmp/personal_income__HID/grouped_features.json\n",
    "gdfs put ~/grouped_features.json /user/vlk/\n",
    "fshare /user/vlk/grouped_features.json\n",
    "\"\"\"\n",
    "grouped_features_adapter = GroupedFeaturesAdapter()\n",
    "grouped_features = grouped_features_adapter.load(\"/home/vlk/\").data\n",
    "log(grouped_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_features_list = []\n",
    "grouped_features_list.append(grouped_features)\n",
    "\n",
    "grouped_features_all, model_column_slices = grouped_features_adapter.merge(*grouped_features_list)\n",
    "\n",
    "rows_transformer = PySparkRows2LearningData(\n",
    "    schema=features_df.schema, \n",
    "    numeric_type=np.float32, \n",
    "    features=grouped_features_all\n",
    ")\n",
    "\n",
    "log(rows_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in chunked_iter(rows, bc.value[\"batch_size\"])\n",
    "chunk = features_rows\n",
    "x = rows_transformer.transform(chunk).x\n",
    "log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return su.check_array(X_res, accept_sparse=sparse_format)\n",
    "\n",
    "x_checked = su.check_array(x, accept_sparse=\"csc\", force_all_finite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = construct(bc.value[\"dataset_adapter\"])\n",
    "dataset = LearnsetAdapter()\n",
    "\n",
    "# prj.adapters.model.oclf.OrdClfLGBMRegressor\n",
    "model_adapter = OrdClfLGBMRegressor().load(\"/home/vlk/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_num = 0\n",
    "for chunk in chunked_iter(features_rows, 1):\n",
    "    chunk_num += 1\n",
    "    if chunk_num < 45:\n",
    "        continue\n",
    "\n",
    "    x = rows_transformer.transform(chunk).x\n",
    "    log((chunk_num, x, x.data), \"current row\")\n",
    "\n",
    "    dataset.set_data(LearningData(x=x[:, model_column_slices[0]]))\n",
    "    prediction_dicts = model_adapter.apply(dataset)\n",
    "\n",
    "    log((prediction_dicts, list(prediction_dicts)), \"prediction\")\n",
    "\n",
    "#     log(np.isfinite(x.data))\n",
    "#     x_checked = su.check_array(x, accept_sparse=\"csc\", force_all_finite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
