{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "from timeit import default_timer as time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes\n",
    "- dm3 RAM limit = 16 GB\n",
    "- cluster nodes RAM limit = 32 GB\n",
    "- HdfsClient который через process.Popen(hadoop ...) жрет 300 мегабайт оперативки\n",
    "```sh\n",
    "# ps -xo uid,rss,pid,cmd #my processes only\n",
    "ps -eo uid,rss,pid,cmd | grep FsShell #all processes\n",
    "978 308844 21882 /etc/alternatives/jre_1.8.0/bin/java -Dproc_fs -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/usr/lib/hadoop/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=/usr/lib/hadoop/lib/native -Dhadoop.log.dir=/usr/lib/hadoop/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/lib/hadoop -Dhadoop.id.str=jenkins-dm -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.fs.FsShell -stat /data/dm/Admining/logs/graph_filter/events/2021-05-26/_SUCCESS\n",
    "```\n",
    "- WebHdfsClient не взлетает, ему не хватает пакета hdfs, который по простому не ставится (pypi недоступен на dm3).\n",
    "\n",
    "Поэтому используем схему с запуском команд hdfs параллельно на экзекуторах спарка, затолкав параметры команд в датафрейм или RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(obj, msg=\"\"):\n",
    "    if msg: print(msg)\n",
    "    print(\"{}, value: `{}`\".format(type(obj), pformat(obj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(os.environ, \"os.environ:\")\n",
    "log(dict(os.environ), \"\\ndict(od.environ):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThreadPool mover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prj.apps.utils.common.fs import HdfsClient\n",
    "from luigi.contrib.hdfs import HdfsClient as LuigiHdfsClient, WebHdfsClient, SnakebiteHdfsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hdfs\n",
    "# !pip install -i http://nexus.k8s.trg.cloud.devmail.ru/repository/pypi-proxy/simple --trusted-host nexus.k8s.trg.cloud.devmail.ru hdfs\n",
    "# !pip install -i http://pkg.trgqa.devmail.ru:8081/repository/pypi-proxy/simple/ --trusted-host pkg.trgqa.devmail.ru hdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = HdfsClient()\n",
    "# hdfs = WebHdfsClient()\n",
    "# hdfs = SnakebiteHdfsClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "class MultithreadedHdfsOps(object):\n",
    "\n",
    "    def __init__(self, hdfs_client, max_threads=128):\n",
    "        self.hdfs = hdfs_client\n",
    "        self.pool = ThreadPool(max_threads)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.pool.close()\n",
    "        self.pool.join()\n",
    "\n",
    "    def move(self, source, dest, callback=None):\n",
    "        return self.pool.apply_async(self._move, args=(source, dest, ), callback=callback)\n",
    "    \n",
    "    def touchz(self, path):\n",
    "        return self.pool.apply_async(self._touchz, args=(path, ))\n",
    "\n",
    "    def _move(self, src, trg):\n",
    "        return self.hdfs.move(src, trg)\n",
    "\n",
    "    def _touchz(self, path):\n",
    "        return self.hdfs.touchz(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = \"hdfs:/user/vlk/\"\n",
    "experiment_root_dir = os.path.join(home_dir, \"files_mover\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir in 6.42792797089 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "hdfs.mkdir(\n",
    "    path=os.path.join(experiment_root_dir, \"files_src\"), \n",
    "    parents=True, \n",
    "    raise_if_exists=False, \n",
    "    remove_if_exists=True  # only for prj version\n",
    ")\n",
    "\n",
    "print(\"mkdir in {} seconds\".format(time() - start))  # mkdir in 6.10173201561 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to touchz 48 files, prepared in 0.000410079956055 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "pool_size=48\n",
    "num_files=48\n",
    "files = [\n",
    "    os.path.join(experiment_root_dir, \"files_src\", \"file_n{}.txt\".format(\"%05d\" % i))\n",
    "    for i in range(1, num_files+1)\n",
    "]\n",
    "print(\"ready to touchz {} files, prepared in {} seconds\".format(len(files), time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# processed 48 files in 13.1580049992 seconds, f/s: 3.64796804215, pool size: 48, files: 48\n",
      "errors: []\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []  # multiprocessing.pool.ApplyResult object\n",
    "with MultithreadedHdfsOps(hdfs, max_threads=pool_size) as mth_hdfs:\n",
    "    for fn in files:\n",
    "        results.append(mth_hdfs.touchz(fn))\n",
    "print(\"# processed {} files in {} seconds, f/s: {}, pool size: {}, files: {}\".format(\n",
    "    len(files), \n",
    "    time() - start, \n",
    "    len(files) / float(time() - start),\n",
    "    pool_size,\n",
    "    num_files\n",
    "))\n",
    "errors = [r.get() for r in results if not r.successful()]\n",
    "print(\"errors: {}\".format(pformat(errors)))\n",
    "# processed 40 files in 10.1468760967 seconds, f/s: 3.942, pool size: 40, files: 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD mover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from timeit import default_timer as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prj.apps.utils.common.fs import HdfsClient\n",
    "# from luigi.contrib.hdfs.hadoopcli_clients import HdfsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from prj.sparkml.scoring import InverseVariabilityTransformer\n",
    "from prj.sparkml.postprocessing import ScoreEqualizeTransformer\n",
    "TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "env_name = os.path.basename(env_dir)\n",
    "env_archive = \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)\n",
    "# Pack executable prj conda environment into zip\n",
    "# !rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = 48\n",
    "executors = cores / 6\n",
    "num_files = cores\n",
    "home_dir = \"hdfs:/user/vlk/\"\n",
    "experiment_root_dir = os.path.join(home_dir, \"files_mover\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export JAVA_HOME=/etc/alternatives/jre_1.8.0 && export PATH=${JAVA_HOME}/bin:$PATH\n",
    "!java -version\n",
    "\n",
    "notes = \"\"\"\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"yarn-client\")\\\n",
    "    .appName(\"DM-AWESOME\")\\\n",
    "    .config(\"spark.yarn.queue\", \"default\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"2G\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"2G\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"200\")\\\n",
    "    .config(\"spark.network.timeout\", \"800s\")\\\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\\\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\\\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2000\")\\\n",
    "    .config(\"spark.driver.memory\", \"2G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "    .config(\"hive.exec.max.dynamic.partitions\", \"10000\")\\\n",
    "    .config(\"hive.exec.max.dynamic.partitions.pernode\", \"10000\")\\\n",
    "    .config(\"spark.jars\", \"hdfs:/lib/dm/prj-transformers-assembly-dev-0.6.0.jar\")\\\n",
    "    .config(\"spark.yarn.dist.archives\", env_archive)\\\n",
    "    .getOrCreate()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master(\"yarn-client\").appName(\"DM-AWESOME\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"spark.app.name\": \"DM-7687-export_hdfs_test\",\n",
    "    \"spark.yarn.queue\": \"dev.priority\",\n",
    "    \"spark.master\": \"yarn-client\",\n",
    "    \"spark.submit.deployMode\": \"cluster\",\n",
    "    \"spark.memory.offHeap.enabled\": \"false\",\n",
    "    \"spark.memory.offHeap.size\": \"1G\",\n",
    "    \"spark.default.parallelism\": cores,\n",
    "    \"spark.sql.shuffle.partitions\": cores,\n",
    "    \"spark.driver.memory\": \"1g\",\n",
    "    \"spark.driver.memoryOverhead\": \"1g\",\n",
    "    \"spark.executor.cores\": cores/executors,\n",
    "    \"spark.executor.instances\": executors,\n",
    "    \"spark.executor.memory\": \"4g\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "    \"spark.dynamicAllocation.maxExecutors\": executors*2,\n",
    "    \"spark.dynamicAllocation.minExecutors\": executors/2,\n",
    "    \"spark.dynamicAllocation.cachedExecutorIdleTimeout\": \"30s\",\n",
    "    \"spark.speculation\": \"true\",\n",
    "    \"hive.exec.dynamic.partition\": \"true\",\n",
    "    \"hive.exec.dynamic.partition.mode\": \"nonstrict\",\n",
    "    \"spark.yarn.maxAppAttempts\": \"1\",\n",
    "    \"spark.jars\": \"hdfs:/lib/dm/prj-transformers-assembly-dev-1.1.0.jar\",\n",
    "    \"spark.yarn.dist.archives\": env_archive,\n",
    "    \"spark.ui.enabled\": \"true\",\n",
    "    #spark.eventLog.dir=hdfs://hacluster/user/spark/applicationHistory\n",
    "    #spark.driver.extraClassPath=/etc/hive/conf\n",
    "    #spark.executor.extraClassPath=/etc/hive/conf\n",
    "    \"spark.eventLog.enabled\": \"true\",\n",
    "    #spark.executorEnv.PYTHONPATH=/usr/lib/spark/python/lib/py4j-current-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\n",
    "    \"spark.hadoop.mapred.output.compress\": \"false\",\n",
    "    \"spark.hadoop.mapred.output.compression.codec\": \"org.apache.hadoop.io.compress.GzipCodec\",\n",
    "    #spark.history.fs.logDirectory=hdfs://hacluster/user/spark/applicationHistory\n",
    "    #spark.jars.ivySettings=/usr/local/etc/ivysettings.xml\n",
    "    #spark.jars.repositories=http://artifactory.hp.rbdev.mail.ru/artifactory/list/maven-mirror\n",
    "    \"spark.logConf\": \"true\",\n",
    "    \"spark.network.timeout\": \"240s\",\n",
    "    #spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS=rbhp-control1.rbdev.mail.ru,rbhp-control2.rbdev.mail.ru\n",
    "    #spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES=http://rm.hadoop.rbdev.mail.ru/proxy/application_1626104146526_35341,http://rm.hadoop.rbdev.mail.ru/proxy/application_1626104146526_35341\n",
    "    #spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS=rm.hadoop.rbdev.mail.ru,rm.hadoop.rbdev.mail.ru\n",
    "    #spark.port.maxRetries=64\n",
    "    \"spark.rdd.compress\": \"true\",\n",
    "    #spark.security.credentials.hbase.enabled=false\n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    #spark.serializer.objectStreamReset=100\n",
    "    \"spark.shuffle.service.enabled\": \"true\",\n",
    "    \"spark.sql.broadcastTimeout\": \"2400\",\n",
    "    \"spark.sql.catalogImplementation\": \"hive\",\n",
    "    \"spark.sql.hive.metastore.jars\": \"/usr/lib/hive/lib/*:/usr/lib/hadoop/client/*\",\n",
    "    \"spark.sql.hive.metastore.version\": \"2.1.1\",\n",
    "    \"spark.sql.orc.compression.codec\": \"zlib\",\n",
    "    #spark.sql.warehouse.dir=hdfs://hacluster/user/hive/warehouse\n",
    "    #spark.ui.filters=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
    "    #spark.ui.port=0\n",
    "    \"spark.yarn.am.waitTime\": \"600s\",\n",
    "    #spark.yarn.app.container.log.dir=/data/disk2/yarn/logs/application_1626104146526_35341/container_e973_1626104146526_35341_01_000001\n",
    "    #spark.yarn.app.id=application_1626104146526_35341\n",
    "    #spark.yarn.historyServer.address=http://rbhp-control5.rbdev.mail.ru:18080\n",
    "    #spark.yarn.isPython=true\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got spark session in 143.72496891 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "spark = SparkSession.builder.config(conf=SparkConf().setAll(conf.items())).getOrCreate()\n",
    "print(\"got spark session in {} seconds\".format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_cores(spark):\n",
    "    return (\n",
    "        len([executor.host() for executor in spark._jsc.sc().statusTracker().getExecutorInfos()]) * \n",
    "        int(spark.sparkContext.getConf().get(\"spark.executor.cores\", \"4\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = 1000  # cores\n",
    "files = [\n",
    "    (\n",
    "        os.path.join(experiment_root_dir, \"files_src\", \"file_n{}.txt\".format(\"%05d\" % i)),\n",
    "        os.path.join(experiment_root_dir, \"files_trg\", \"file_n{}.txt\".format(\"%05d\" % i))\n",
    "    )\n",
    "    for i in range(1, num_files+1)\n",
    "]\n",
    "# log(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = HdfsClient()\n",
    "hdfs.remove(\n",
    "    os.path.join(experiment_root_dir, \"files_trg\", \"\"), \n",
    "    recursive=True, \n",
    "    skip_trash=True, \n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# processed 1000 files in 422.594970942 seconds, f/s: 2.36633197073, pool size: 48\n",
      "errors: []\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []  # multiprocessing.pool.ApplyResult object\n",
    "with MultithreadedHdfsOps(hdfs, max_threads=cores) as mth_hdfs:\n",
    "    for fn, _ in files:\n",
    "        results.append(mth_hdfs.touchz(fn))\n",
    "print(\"# processed {} files in {} seconds, f/s: {}, pool size: {}\".format(\n",
    "    len(files), \n",
    "    time() - start, \n",
    "    len(files) / float(time() - start),\n",
    "    cores\n",
    "))\n",
    "errors = [r.get() for r in results if not r.successful()]\n",
    "print(\"errors: {}\".format(pformat(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luigi.contrib.hdfs.error import HDFSCliError\n",
    "def move_files(split, iter):\n",
    "    hdfs = HdfsClient()\n",
    "    for src, trg in iter:\n",
    "        retcode = 0\n",
    "        try:\n",
    "            out = hdfs.move(src, trg)\n",
    "        except HDFSCliError as e:\n",
    "            out = e.message\n",
    "            retcode = e.returncode\n",
    "        yield (\n",
    "            src,\n",
    "            trg,\n",
    "            retcode,\n",
    "            out,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores before job: 30\n",
      "# processed 1000 files in 258.257082939 seconds, f/s: 3.87211062714, cores at the end: 102\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "rdd = spark.sparkContext.parallelize(files, num_files)\n",
    "print(\"Cores before job: {}\".format(actual_cores(spark)))\n",
    "results = rdd.mapPartitionsWithIndex(move_files).collect()\n",
    "print(\"# processed {} files in {} seconds, f/s: {}, cores at the end: {}\".format(\n",
    "    len(files), \n",
    "    time() - start, \n",
    "    len(files) / float(time() - start),\n",
    "    actual_cores(spark)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = []\n",
    "fail = []\n",
    "for src, trg, retcode, out in results:\n",
    "    if retcode == 0:\n",
    "        success.append(src)\n",
    "    else:\n",
    "        fail.append(src)\n",
    "        log(\"Failed to move file: `{}`, retcode: {},\\nmessage: `{}`\".format(src, retcode, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(success) != len(files):\n",
    "    log(\"Operation failed, files:\\n{}\".format(pformat(fail)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log(results)\n",
    "errs = \"\"\"\n",
    "('hdfs:/user/vlk/files_mover/files_src/file_n00001.txt',\n",
    "  'hdfs:/user/vlk/files_mover/files_trg/file_n00001.txt',\n",
    "  0,\n",
    "  None\n",
    "),\n",
    "\n",
    "('hdfs:/user/vlk/files_mover/files_src/file_n00001.txt',\n",
    "  'hdfs:/user/vlk/files_mover/files_trg/file_n00001.txt',\n",
    "  1,\n",
    "  \"Command ['hadoop', 'fs', '-mv', 'hdfs:/user/vlk/files_mover/files_src/file_n00001.txt', 'hdfs:/user/vlk/files_mover/files_trg/file_n00001.txt'] failed [exit code 1]\\n---stdout---\\n\\n---stderr---\\nmv: `hdfs:///user/vlk/files_mover/files_trg/file_n00001.txt': File exists\\n------------\"\n",
    ")\n",
    "  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"results: {} {}\\n{}\".format(type(results), type(results[0]), pformat(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark._jsc.sc()\n",
    "executors = [executor.host() for executor in sc.statusTracker().getExecutorInfos()]\n",
    "log(executors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>, value: `u'6'`\n"
     ]
    }
   ],
   "source": [
    "executor_cores = spark.sparkContext.getConf().get(\"spark.executor.cores\", \"4\")\n",
    "log(executor_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual cores: 30\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "from operator import and_\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "from pyspark.sql.types import MapType, ArrayType, FloatType, StringType, NumericType\n",
    "from luigi.contrib.hdfs import HdfsTarget\n",
    "\n",
    "from prj.apps.utils.common import DT_FORMAT\n",
    "from prj.apps.utils.common.fs import HdfsClient\n",
    "from prj.apps.utils.common.hive import select_clause, find_partitions\n",
    "\n",
    "if six.PY3:\n",
    "    from functools import reduce  # make flake8 happy\n",
    "\n",
    "DEFAULT_MAX_DT_DIFF = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools as it\n",
    "\n",
    "import six\n",
    "\n",
    "from luigi.contrib.hive import run_hive_cmd\n",
    "\n",
    "from dmcore.utils.common import to_str\n",
    "from prj.common.hive import HiveMetastoreClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expand_partitions(**partition_conf):\n",
    "    \"\"\"Convert a partition configuration to a list of partition dicts via Cartesian product.\n",
    "\n",
    "    :param partition_conf: key is a partition field name, value is one or several partition values.\n",
    "        All values are converted to strings to meet Hive metastore response format.\n",
    "\n",
    "    :return: a list of dicts, partition descriptions.\n",
    "    :rtype: list[dict[str,str]].\n",
    "\n",
    "    :Example:\n",
    "\n",
    "    >>> expand_partitions(dt=\"2020-01-01\", uid_type=[\"VKID\", \"OKID\", \"EMAIL\"])\n",
    "    [\n",
    "        {\"dt\": \"2020-01-01\", uid_type: \"VKID\"},\n",
    "        {\"dt\": \"2020-01-01\", uid_type: \"OKID\"},\n",
    "        {\"dt\": \"2020-01-01\", uid_type: \"EMAIL\"}\n",
    "    ]\n",
    "    >>> expand_partitions()  # One empty partition spec\n",
    "    [{}]\n",
    "    >>> expand_partitions(foo=\"foo\", bar=[])  # Empty Cartesian product\n",
    "    []\n",
    "    \"\"\"\n",
    "    return [\n",
    "        dict(zip(partition_conf.keys(), map(to_str, values)))\n",
    "        for values in it.product(*(v if isinstance(v, (list, tuple, set)) else [v] for v in partition_conf.values()))\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_partitions(database, table, partition_conf, min_dt=None, max_dt=None, match_mode=\"all\"):\n",
    "    \"\"\"Get a list of Hive table partitions with a given configuration.\n",
    "\n",
    "    .. note:: A given Hive table is assumed to have \"dt\" partitioning column.\n",
    "\n",
    "    :param str database: a database name.\n",
    "    :param str table: a table name.\n",
    "    :param partition_conf: a dictionary with partitions configuration, where key is a partition column name, and value\n",
    "        is either a partition column value, or a list of values. A special partitioning column is \"dt\", for which a key\n",
    "        is optional.\n",
    "        If \"dt\" is missing, then the last date value that matches ``partition_conf`` is selected.\n",
    "        If \"dt\" has a single value, then only partitions that match the rest ``partition_conf`` for this date are\n",
    "        selected.\n",
    "        If \"dt\" has multiple values, then only partitions that match the rest ``partition_conf`` for at least one of\n",
    "        the dates in a list are selected.\n",
    "        There are supported several matching modes, see ``match_mode`` parameter.\n",
    "    :type partition_conf: dict[str, typing.Union[str, list[str]]].\n",
    "\n",
    "    :param str min_dt: (optional) a date, for example, \"2018-10-01\". It is used only when ``dt`` is not set.\n",
    "        In this case the last date value >= ``min_dt`` that matches ``partition_conf`` is selected.\n",
    "    :param str max_dt: (optional) a date, for example, \"2018-10-10\". It is used only when ``dt`` is not set.\n",
    "        In this case the last date value <= ``max_dt`` that matches ``partition_conf`` is selected.\n",
    "\n",
    "    :param str match_mode: specifies, how a date is considered to match ``partition_conf`` (here we consider this\n",
    "        configuration without \"dt\" key). Possible values are:\n",
    "\n",
    "        - \"all\" (default), a date matches ``partition_conf`` when and only when all partitions from ``partition_conf``\n",
    "            Cartesian product exist for that date.\n",
    "        - \"any\", a date matches ``partition_conf``, if there exists at least one partition from ``partition_conf``\n",
    "            Cartesian product for that date.\n",
    "\n",
    "    :return: a list of dicts, partition descriptions.\n",
    "    :rtype: list[dict[str,str]].\n",
    "\n",
    "    :Example:\n",
    "\n",
    "    >>> find_partitions(\"ds_auditories\", \"xlal_sample\", {\"audience_name\": \"12345\", \"uid_type\": [\"VKID\", \"OKID\"]})\n",
    "    [\n",
    "        {\"audience_name\": \"12345\", \"category\": \"positive\", \"dt\": \"2020-04-20\", \"uid_type\": \"VKID\"},\n",
    "        {\"audience_name\": \"12345\", \"category\": \"positive\", \"dt\": \"2020-04-20\", \"uid_type\": \"OKID\"},\n",
    "        {\"audience_name\": \"12345\", \"category\": \"negative\", \"dt\": \"2020-04-20\", \"uid_type\": \"VKID\"},\n",
    "        {\"audience_name\": \"12345\", \"category\": \"negative\", \"dt\": \"2020-04-20\", \"uid_type\": \"OKID\"}\n",
    "    ]\n",
    "    >>> find_partitions(\"cdm_scoring\", \"specified_socdem\", {\"uid_type\": \"IDFA\", \"dt\": [\"1999-01-01\", \"2020-11-17\"]})\n",
    "    [{\"dt\": \"2020-11-17\", \"uid_type\": \"IDFA\"}]\n",
    "    >>> find_partitions(\"ds_auditories\", \"xlal_sample\", {\"audience_name\": \"12345\", \"uid_type\": []})\n",
    "    []\n",
    "    \"\"\"\n",
    "    if match_mode not in {\"all\", \"any\"}:\n",
    "        raise ValueError(\"Unsupported match_mode='{}'\".format(match_mode))\n",
    "\n",
    "    hive_client = HiveMetastoreClient()\n",
    "\n",
    "    if \"dt\" not in hive_client.get_partition_names(database=database, table=table):\n",
    "        raise TypeError(\"A table {}.{} has no 'dt' partitioning column\".format(database, table))\n",
    "\n",
    "    dt = partition_conf.get(\"dt\")\n",
    "    existing_partitions = hive_client.get_partitions(database=database, table=table)\n",
    "    log(existing_partitions, \"table partitions:\")\n",
    "    expanded_conf_partitions = expand_partitions(**{k: v for k, v in six.iteritems(partition_conf) if k != \"dt\"})\n",
    "    log(expanded_conf_partitions, \"expanded partitions:\")\n",
    "\n",
    "    if len(expanded_conf_partitions) == 0:\n",
    "        log(None, \"expanded partitions list is empty:\")\n",
    "        return []\n",
    "\n",
    "    if dt is None:\n",
    "        dts_sets = (\n",
    "            set(ep[\"dt\"] for ep in existing_partitions if set(p.items()).issubset(ep.items()))\n",
    "            for p in expanded_conf_partitions\n",
    "        )\n",
    "        dts_sets = list(dts_sets)\n",
    "        log(dts_sets, \"list of dt sets:\")\n",
    "        \n",
    "        if match_mode == \"all\":\n",
    "            dts = set.intersection(*dts_sets)\n",
    "        else:\n",
    "            dts = set.union(*dts_sets)\n",
    "\n",
    "        if min_dt or max_dt:\n",
    "            dts = [dt for dt in dts if (min_dt is None or min_dt <= dt) and (max_dt is None or dt <= max_dt)]\n",
    "\n",
    "        if not dts:\n",
    "            log(None, \"dt list not found:\")\n",
    "            return []\n",
    "        else:\n",
    "            dts = [max(dts)]\n",
    "\n",
    "    else:\n",
    "        dts = [dt] if isinstance(dt, six.string_types) else dt\n",
    "\n",
    "    log(dts, \"list of dt:\")\n",
    "    result_partitions = []\n",
    "\n",
    "    for dt in dts:\n",
    "        dt_partitions = []\n",
    "\n",
    "        for p in expanded_conf_partitions:\n",
    "            p = dict(dt=dt, **p)\n",
    "            matched_existing_partitions = [ep for ep in existing_partitions if set(p.items()).issubset(ep.items())]\n",
    "            log(p, \"needed partition:\")\n",
    "            log(matched_existing_partitions, \"matched partitions:\")\n",
    "\n",
    "            if (match_mode == \"all\") and not matched_existing_partitions:\n",
    "                dt_partitions = []\n",
    "                break\n",
    "\n",
    "            dt_partitions.extend(matched_existing_partitions)\n",
    "\n",
    "        result_partitions.extend(dt_partitions)\n",
    "\n",
    "    return result_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportUniversalFeatureApp(object):\n",
    "\n",
    "    config_example = {\n",
    "        \"target_dt\": \"2021-06-04\",\n",
    "        \"max_dt_diff\": 30,  # optional\n",
    "        \"source\": {\n",
    "            \"db\": \"ds_scoring\",\n",
    "            \"table\": \"dm_universal_feature\",\n",
    "            \"partitions\": [\n",
    "                {\"feature_name\": \"user_app_cats_installed\", \"uid_type\": \"GAID\"},\n",
    "                {\"feature_name\": \"user_app_cats_installed\", \"uid_type\": \"IDFA\"},\n",
    "                {\"feature_name\": \"dm8792_showed_urls\", \"uid_type\": \"VKID\"},\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def prepare_config(self, config):\n",
    "        # replace declared partitions list with list of existing partitions\n",
    "        def make_partition_conf(plist):\n",
    "            p_conf = defaultdict(set)\n",
    "            for p in plist:\n",
    "                for k, v in six.iteritems(p):\n",
    "                    p_conf[k].add(v)\n",
    "            return p_conf\n",
    "    \n",
    "        def dt_minus(dt, days):\n",
    "            date = datetime.datetime.strptime(dt, DT_FORMAT).date()\n",
    "            delta = datetime.timedelta(days=int(days))\n",
    "            return (date - delta).isoformat()\n",
    "    \n",
    "        target_dt = config[\"target_dt\"]\n",
    "        min_dt = dt_minus(target_dt, config.get(\"max_dt_diff\", DEFAULT_MAX_DT_DIFF))\n",
    "        log(min_dt, \"min dt:\")\n",
    "        source_conf = config[\"source\"]\n",
    "        partition_conf = make_partition_conf(source_conf.get(\"partitions\", []))\n",
    "        log(partition_conf, \"partition conf dict:\")\n",
    "        source_conf[\"partitions\"] = find_partitions(\n",
    "            source_conf[\"db\"], source_conf[\"table\"], partition_conf, min_dt=min_dt, max_dt=target_dt, match_mode=\"any\"\n",
    "        )\n",
    "    \n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = ExportUniversalFeatureApp()\n",
    "cfg = app.prepare_config(app.config_example)\n",
    "log(cfg, \"actual cfg:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from prj.sparkml.scoring import InverseVariabilityTransformer\n",
    "from prj.sparkml.postprocessing import ScoreEqualizeTransformer\n",
    "\n",
    "TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "env_name = os.path.basename(env_dir)\n",
    "env_archive = \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack executable prj conda environment into zip\n",
    "!rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la {TMP_ENV_BASEDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with prj conda environment and JVM extensions\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"yarn-client\")\\\n",
    "    .appName(\"DM-AWESOME\")\\\n",
    "    .config(\"spark.yarn.queue\", \"default\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"2G\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"2G\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"200\")\\\n",
    "    .config(\"spark.network.timeout\", \"800s\")\\\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\\\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\\\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2000\")\\\n",
    "    .config(\"spark.driver.memory\", \"2G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "    .config(\"hive.exec.max.dynamic.partitions\", \"10000\")\\\n",
    "    .config(\"hive.exec.max.dynamic.partitions.pernode\", \"10000\")\\\n",
    "    .config(\"spark.jars\", \"hdfs:/lib/dm/prj-transformers-assembly-dev-0.6.0.jar\")\\\n",
    "    .config(\"spark.yarn.dist.archives\", env_archive)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TEMPORARY FUNCTION gmin as 'prj.hive.udaf.features.GenericMinUDAF'\")\n",
    "spark.sql(\"CREATE TEMPORARY FUNCTION gmax as 'prj.hive.udaf.features.GenericMaxUDAF'\")\n",
    "spark.sql(\"CREATE TEMPORARY FUNCTION gavg as 'prj.hive.udaf.features.GenericAvgUDAF'\")\n",
    "spark.sql(\"CREATE TEMPORARY FUNCTION gsum as 'prj.hive.udaf.features.GenericSumUDAF'\")\n",
    "spark.sql(\"CREATE TEMPORARY FUNCTION most_freq AS 'prj.hive.udaf.features.GenericMostFreqUDAF'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- uid_type: string (nullable = true)\n",
      " |-- aggregation_period: string (nullable = true)\n",
      " |-- audience_name: string (nullable = true)\n",
      " |-- shows_counts: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- expected: double (nullable = true)\n",
      "\n",
      "+---+--------+------------------+--------------+------------------+------------------+\n",
      "|uid|uid_type|aggregation_period| audience_name|      shows_counts|          expected|\n",
      "+---+--------+------------------+--------------+------------------+------------------+\n",
      "|  a|    OKID|            3_days|ANTIFRAUD_SHOW|[43.0, 50.0, 19.0]|1.2100051013643267|\n",
      "|  c|     VID|            3_days|ANTIFRAUD_SHOW|[49.0, 30.0, 10.0]|0.9048109505117552|\n",
      "|  b|    VKID|            3_days|ANTIFRAUD_SHOW|[40.0, 20.0, 15.0]| 2.876131281346695|\n",
      "|  d|     VID|            3_days|       FOO_BAR|          [23.0,,]|0.0476731294622796|\n",
      "+---+--------+------------------+--------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as sqlfn\n",
    "text = \"\"\"\n",
    "            |a  |OKID    |3_days            |ANTIFRAUD_SHOW|[43.0, 50.0, 19.0] |1.2100051013643267\n",
    "            |c  |VID     |3_days            |ANTIFRAUD_SHOW|[49.0, 30.0, 10.0] |0.9048109505117552\n",
    "            |b  |VKID    |3_days            |ANTIFRAUD_SHOW|[40.0, 20.0, 15.0] |2.8761312813466950\n",
    "            |d  |VID     |3_days            |FOO_BAR       |[23.0,,]           |0.0476731294622796     \n",
    "        \"\"\"\n",
    "df = (\n",
    "    spark.createDataFrame([\n",
    "            tuple([item.strip() for item in row.split(\"|\") if item.strip()]) for row in text.strip().split(\"\\n\")\n",
    "    ]).toDF(\"uid\", \"uid_type\", \"aggregation_period\", \"audience_name\", \"counts\", \"expected\")\n",
    ").withColumn(\n",
    "            \"shows_counts\",\n",
    "            sqlfn.expr(r\"split(regexp_replace(counts, '(\\\\[|\\\\])', ''), ',')\")\n",
    ").selectExpr(\n",
    "            \"uid\", \"uid_type\", \"aggregation_period\", \"audience_name\",\n",
    "            \"cast(shows_counts as array<double>) as shows_counts\",\n",
    "            \"cast(expected as double) as expected\"\n",
    ")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- uid_type: string (nullable = true)\n",
      " |-- aggregation_period: string (nullable = true)\n",
      " |-- audience_name: string (nullable = true)\n",
      " |-- shows_counts: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- expected: double (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n",
      "+---+--------+------------------+--------------+------------------+------------------+-------------------+\n",
      "|uid|uid_type|aggregation_period| audience_name|      shows_counts|          expected|              score|\n",
      "+---+--------+------------------+--------------+------------------+------------------+-------------------+\n",
      "|  a|    OKID|            3_days|ANTIFRAUD_SHOW|[43.0, 50.0, 19.0]|1.2100051013643267| 1.2100051013643267|\n",
      "|  c|     VID|            3_days|ANTIFRAUD_SHOW|[49.0, 30.0, 10.0]|0.9048109505117552| 0.9048109505117552|\n",
      "|  b|    VKID|            3_days|ANTIFRAUD_SHOW|[40.0, 20.0, 15.0]| 2.876131281346695|  2.876131281346695|\n",
      "|  d|     VID|            3_days|       FOO_BAR|          [23.0,,]|0.0476731294622796|0.04767312946227962|\n",
      "+---+--------+------------------+--------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prj.sparkml.scoring import InverseVariabilityTransformer\n",
    "model = InverseVariabilityTransformer(\n",
    "    inputCol=\"shows_counts\",\n",
    "    groupColumns=\"audience_name, uid_type, aggregation_period\",\n",
    "    weightValue=0.01\n",
    ").fit(df)\n",
    "result = model.setOutputCol(\"score\").transform(df)\n",
    "result.printSchema()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- score_raw_train: double (nullable = true)\n",
      " |-- score_raw: double (nullable = true)\n",
      " |-- expected: double (nullable = true)\n",
      "\n",
      "+---+---------------+---------+----------+\n",
      "|uid|score_raw_train|score_raw|  expected|\n",
      "+---+---------------+---------+----------+\n",
      "|  a|            0.3|      0.1|       0.0|\n",
      "|  b|            0.7|     3.14|0.27968233|\n",
      "|  c|           13.0|     26.0|0.74796144|\n",
      "|  d|           17.0|     28.0|       1.0|\n",
      "|  e|           27.0|     15.0| 0.4982242|\n",
      "+---+---------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "                (\"a\", 0.3,  0.1,  0.0),\n",
    "                (\"b\", 0.7,  3.14, 0.27968233),\n",
    "                (\"c\", 13.0, 26.0, 0.74796144),\n",
    "                (\"d\", 17.0, 28.0, 1.0),\n",
    "                (\"e\", 27.0, 15.0, 0.4982242)\n",
    "            ]).toDF(\"uid\", \"score_raw_train\", \"score_raw\", \"expected\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- score_raw_train: double (nullable = true)\n",
      " |-- score_raw: double (nullable = true)\n",
      " |-- expected: double (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n",
      "+---+---------------+---------+----------+------------------+\n",
      "|uid|score_raw_train|score_raw|  expected|             score|\n",
      "+---+---------------+---------+----------+------------------+\n",
      "|  a|            0.3|      0.1|       0.0|               0.0|\n",
      "|  b|            0.7|     3.14|0.27968233|0.2796823308046389|\n",
      "|  c|           13.0|     26.0|0.74796144| 0.747961435953159|\n",
      "|  d|           17.0|     28.0|       1.0|               1.0|\n",
      "|  e|           27.0|     15.0| 0.4982242| 0.498224197874749|\n",
      "+---+---------------+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prj.sparkml.postprocessing import ScoreEqualizeTransformer\n",
    "model = ScoreEqualizeTransformer(\n",
    "  inputCol=\"score_raw_train\",\n",
    "  groupColumns=\"\",\n",
    "  sampleSize=100000,\n",
    "  numBins=10000,\n",
    "  noiseValue=1e-4,\n",
    "  epsValue=1e-3,  \n",
    "  randomValue=0.5\n",
    ").fit(df)\n",
    "result = model.setInputCol(\"score_raw\").setOutputCol(\"score\").transform(df)\n",
    "result.printSchema()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
