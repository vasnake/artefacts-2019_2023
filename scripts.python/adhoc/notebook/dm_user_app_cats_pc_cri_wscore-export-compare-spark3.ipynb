{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRG-73014 UniversalFeatures export compare\n",
    "Сравнение результатов оригинального экспорта и нового (Spark3, Python3), rbhp-dm3 conda3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pay attention to log(sys.version_info), you have to `conda-pack` the same version of python\n",
    "\n",
    "def log(obj, msg=\"\"):\n",
    "    if msg: print(msg)\n",
    "    print(\"type: {}\\ndata: {}\".format(type(obj), pformat(obj, indent=1, width=1)))\n",
    "\n",
    "log(os.environ, \"os.environ\")\n",
    "print()\n",
    "log(dict(os.environ), \"dict(os.environ)\")\n",
    "print()\n",
    "log(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_env_example = \"\"\"\n",
    "export SPARK_HOME=/usr/lib/spark3\n",
    "export PYTHONPATH=/usr/lib/spark3/python/lib/py4j-current-src.zip:/usr/lib/spark3/python\n",
    "export PYSPARK_PYTHON=/usr/bin/python3\n",
    "export PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n",
    "export PATH=/etc/alternatives/jre_1.8.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/data/anaconda3/bin:/data/anaconda2/bin\n",
    "\"\"\"\n",
    "\n",
    "for path in [\n",
    "    \"/data/anaconda2/bin\",\n",
    "    \"/usr/lib/spark/python\", \n",
    "    \"/usr/lib/spark/python/lib/py4j-current-src.zip\"\n",
    "]:\n",
    "    try:\n",
    "        sys.path.remove(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "sys.path.append(\"/usr/lib/spark3/python/lib/py4j-current-src.zip\")\n",
    "sys.path.append(\"/usr/lib/spark3/python\")\n",
    "os.environ[\"PATH\"] = f\"/usr/lib/spark3/:/usr/lib/spark3/python/lib/py4j-current-src.zip:/data/anaconda3/bin:{os.environ.get('PATH')}\"\n",
    "\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://rbhp-proxy.i:3128\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/lib/spark3\"\n",
    "del os.environ[\"PYSPARK_PYTHON\"]\n",
    "del os.environ[\"PYTHONPATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare env on host\n",
    "\n",
    "# https://databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html\n",
    "# https://conda.github.io/conda-pack/spark.html\n",
    "# !export HTTPS_PROXY=http://rbhp-proxy.i:3128\n",
    "_script=\"\"\"\n",
    " export HTTPS_PROXY=http://rbhp-proxy.i:3128\n",
    " which conda\n",
    " conda -V\n",
    " conda create -y -n pyspark_conda_env python=3.7\n",
    " conda activate pyspark_conda_env\n",
    " conda install conda-pack\n",
    " conda pack -f -o pyspark_conda_env.tar.gz\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://conda.github.io/conda-pack/spark.html\n",
    "# !export PYSPARK_PYTHON=./environment/bin/python\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n",
    "# $ PYSPARK_PYTHON=./environment/bin/python \\\n",
    "# spark-submit \\\n",
    "# --master yarn \\\n",
    "# --deploy-mode cluster \\\n",
    "# --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/bin/python\n",
    "# --archives pyspark_conda_env.tar.gz#environment \\\n",
    "\n",
    "# !export PYSPARK_DRIVER_PYTHON=/home/vlk/.conda/envs/pyspark_conda_env/bin/python\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/vlk/.conda/envs/pyspark_conda_env/bin/python\"\n",
    "# $ PYSPARK_DRIVER_PYTHON=`which python` \\\n",
    "# PYSPARK_PYTHON=./environment/bin/python \\\n",
    "# spark-submit \\\n",
    "# --master yarn \\\n",
    "# --deploy-mode client \\\n",
    "# --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/bin/python \\\n",
    "# --archives pyspark_conda_env.tar.gz#environment \\\n",
    "\n",
    "# .config(\"spark.yarn.dist.archives\", \"pyspark_conda_env.tar.gz#environment\")\n",
    "# .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", \"./environment/bin/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for prj conda env\n",
    "# Pack executable prj conda environment into zip\n",
    "\n",
    "def _create_env_archive():\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "    env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "    env_name = os.path.basename(env_dir)\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)\n",
    "    # you need this only once time!\n",
    "    # !rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n",
    "    return \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "    \n",
    "# env_archive = _create_env_archive()\n",
    "# log(env_archive)\n",
    "\n",
    "# .config(\"spark.yarn.dist.archives\", env_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with [prj] conda environment and JVM extensions\n",
    "\n",
    "# `spark-submit ... --driver-java-options \"-Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\"`\n",
    "# \"spark.driver.extraJavaOptions\", \"-Xss10M\"\n",
    "# catalyst SO while building parts. filter expression\n",
    "ejo = \"-Dlog4j.configuration=file:/home/vlk/driver2_log4j.properties\"\n",
    "\n",
    "queue = \"root.default\"\n",
    "\n",
    "# 1000 GB => 4000 part. => 8000 real part.\n",
    "# 4 TB of data\n",
    "# sssp = 4 * 4 * 1024\n",
    "# 50 GB\n",
    "# sssp = 50.0 * 4 * 2\n",
    "# 10 GB\n",
    "sssp = 10.0 * 4 * 2\n",
    "\n",
    "spark = (\n",
    "SparkSession.builder\n",
    "    .master(\"yarn\")\n",
    "    .appName(\"TRG-73014-test-ipynb\")\n",
    "    .config(\"spark.yarn.dist.archives\", \"pyspark_conda_env.tar.gz#environment\")\n",
    "    .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", \"./environment/bin/python\")\n",
    "    .config(\"spark.yarn.queue\", queue)\n",
    "    .config(\"spark.sql.shuffle.partitions\", int(math.ceil(sssp)))\n",
    "    .config(\"spark.driver.extraJavaOptions\", ejo)\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"8G\")\n",
    "    .config(\"spark.executor.cores\", \"6\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2G\")\n",
    "    .config(\"spark.driver.memory\", \"4G\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\n",
    "    .config(\"spark.speculation\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"512\")\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\n",
    "    .config(\"spark.network.timeout\", \"800s\")\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition\", \"true\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions\", \"1000000\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions.pernode\", \"100000\")\n",
    "    .config(\"spark.hadoop.hive.metastore.client.socket.timeout\", \"300s\")\n",
    "    .config(\"spark.ui.enabled\", \"true\")\n",
    "    .config(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")    \n",
    "    .getOrCreate()\n",
    ")\n",
    "#     .config(\"spark.jars\", \"hdfs:/lib/dm/prj-transformers-assembly-dev-1.5.2.jar\")\n",
    "\n",
    "sql_ctx = SQLContext(spark.sparkContext)\n",
    "(spark, sql_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of env. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pprint\n",
    "from pprint import pformat\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from operator import and_\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "if six.PY3:\n",
    "    from functools import reduce  # make flake8 happy\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import DataFrame, SQLContext\n",
    "from pyspark.sql.types import (\n",
    "    MapType, ArrayType, FloatType, DoubleType, StringType, StructType, IntegralType, IntegerType, NumericType\n",
    ")\n",
    "from pyspark.sql.utils import CapturedException\n",
    "from pyspark.ml.wrapper import JavaWrapper\n",
    "\n",
    "# import luigi\n",
    "# from luigi.contrib.hdfs import HdfsTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomUDFLibrary(spark, \"hdfs:/lib/dm/prj-transformers-assembly-dev-1.5.0.jar\").register_all_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, message=\"dataframe\", nlines=20, truncate=False, heavy=True):\n",
    "    if heavy:\n",
    "        print(\"\\n{}:\".format(message))\n",
    "        df.printSchema()\n",
    "        return df\n",
    "\n",
    "    print(\"\\n{}, rows: {}:\".format(message, df.count()))\n",
    "    df.printSchema()\n",
    "    df.show(nlines, truncate)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UniversalFeatures compare emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdfs head -n 3 /export/target/universal_features/dm_extended_features_v2/2021-12-31/part-00000-85c2b8eb-591a-4d44-9ff7-a6c198a7191e-c000.csv\n",
    "old_file_text = \"\"\"\n",
    "user;num:ext_feature2\n",
    "vk:601804359;0:0.133850812912,1:4723.9375,2:0.869369387627,3:0.10000000149,4:0.0500000007451,5:0.0500000007451,6:0.0500000007451,10:3.40000009537,11:5.40000009537,12:2.59999990463,13:15385.1669922,14:8117.16650391,15:203079.578125,16:242712.25,17:320989.59375,18:1333384.875,19:13013.25,20:21976.9160156,21:13660.1669922,22:1720.16662598,23:26525.25,24:0.0434782616794,25:0.0434782616794,26:0.0434782616794,27:0.0869565233588,28:0.0434782616794,29:0.0869565233588,30:0.0434782616794,31:1583.72814941,32:44.6000022888,33:114.233337402,34:393.619049072,35:2361.46655273,36:0.0625,37:0.883668005466,38:23392.9667969,1330:3,1335:3,1343:3,1347:3,1097:3,1243:3,1548:3,1533:3\n",
    "vk:72293765;0:0.374163866043,1:405.89855957,2:0.920303463936,3:0.0774647891521,4:0.0211267601699,5:0.0140845067799,6:0.0845070406795,7:0.866715550423,8:0.709127545357,9:0.510675251484,10:5.32258081436,11:1.13709676266,12:3.30645155907,13:15810.6582031,14:8899.97558594,15:197297.53125,16:223353.984375,17:317720.75,18:1335009.125,19:12406.796875,20:21937.2792969,21:9697.89746094,22:1478.60449219,23:23335.1816406,24:0.0206896550953,25:0.00689655169845,26:0.00689655169845,27:0.110344827175,28:0.00689655169845,29:0.0206896550953,30:0.00689655169845,31:1944.44030762,32:45.0097999573,33:135.024597168,34:742.001220703,35:2077.11474609,36:0.253623187542,37:0.919775247574,38:21882.328125,1547:452,2120:3,1097:2421,1122:25,2127:1,2130:3,2131:3,2132:3,2138:208,2145:498,2146:8,2147:3,2066:5,2070:9,2187:2,2188:18,2157:1,2212:7,2213:5,2290:5,2218:1,2224:1,2231:6,2232:35,2233:53,2237:2,2241:2,2245:78,1057:12,2248:7,2259:1,2261:8,1243:73,1247:1651,1062:132,2278:1,2283:1,2284:10,2285:1,2287:1,2288:1,1266:144,2291:5,2295:5,2296:3,2305:16,2307:1,2310:2,2316:19,2318:2,2319:14,2321:2,1302:6,2333:60,2334:12,2335:33,1319:734,2345:24,2346:2,2348:2,2350:1,1330:2593,1333:15,2358:6,1335:2593,1337:15,1338:491,2363:2,1340:243,1343:1874,1346:33,1347:1723,1348:452,2359:2,2384:1,2413:24,2415:2,2417:119,2418:2,2419:20,2426:233,2428:2,2429:69,2430:80,2431:1,1426:33,1548:1769,1453:73,2379:2,2012:1,2016:1,2021:3,2022:35,2023:2,2024:1,2029:1,2036:3,2217:1,1533:1271\n",
    "ok:571813119726;1547:20,1161:10,1345:20,1335:20,1343:20,1051:10,1330:20.01,1061:10.0,1253:20.0,1348:20.0\n",
    "\"\"\"\n",
    "# gdfs head -n 3 /data/dm/prj/dev/apps/export/universal_features/dm_extended_features_v2/2022-01-09/part-00000-1929771c-58d1-4a7a-938b-775fc92beaae-c000.csv\n",
    "new_file_text = \"\"\"\n",
    "user;num:ext_feature2\n",
    "ok:571813119726;1547:20.00001,1161:10.0,1345:20.0,1335:20.0,1343:20.0,1051:10.0,1330:20.0,1061:10.0,1253:20.0,1348:20.0\n",
    "ok:590558313987;12:0.0,1547:48.0,8:0.40449977,4:0.2,1221:10.0,11:1.0,9:0.40449977,1323:14.0,26:0.125,37:0.24138813,24:0.125,1265:23.0,5:0.2,10:0.0,1345:48.0,1335:48.0,6:0.2,1320:1.0,36:0.0,1343:38.0,1:2792.0,25:0.125,1062:32.0,1051:16.0,0:0.6542008,27:0.125,2:0.24138813,1330:48.0,30:0.125,7:0.40449977,29:0.125,1338:10.0,3:0.2,28:0.125,1348:48.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_lines(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [x.strip() for x in lines if len(x.strip()) > 0]\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(text, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_hdfs(text, hdfs_dir, file_name):\n",
    "    from backports import tempfile\n",
    "    from prj.apps.utils.common.fs import HdfsClient\n",
    "    hdfs = HdfsClient()\n",
    "    success_filename = \"_SUCCESS\"\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_local_dir:\n",
    "        write_text(drop_empty_lines(text), os.path.join(tmp_local_dir, file_name))\n",
    "        hdfs.mkdir(hdfs_dir, remove_if_exists=True)\n",
    "        hdfs.put(tmp_local_dir, hdfs_dir, content_only=True)\n",
    "    hdfs.touchz(os.path.join(hdfs_dir, success_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_hdfs(old_file_text, \"hdfs:/user/vlk/test/TRG-73014/dm_extended_features_v2/old/\", \"features.csv\")\n",
    "save_to_hdfs(new_file_text, \"hdfs:/user/vlk/test/TRG-73014/dm_extended_features_v2/new/\", \"features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of emulation, pay attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"hdfs:/user/vlk/test/TRG-73014/dm_extended_features_v2/old/\"\n",
    "# path=\"hdfs:/export/target/universal_features/dm_extended_features_v2/2021-12-31/\"\n",
    "\n",
    "# path = \"hdfs:/export/target/universal_features/dm_user_af_shows/2022-02-08/*.csv\"\n",
    "# user;num:mean_shows_per_day;num:visit_probability;num:total_shows_cnt;num:mean;num:std;num:hours\n",
    "\n",
    "path = \"hdfs:/export/target/universal_features/dm_user_app_cats_pc_cri_wscore/2022-02-10/*.csv\"\n",
    "# user;num:dm_user_app_cats_pc_cri_wscore\n",
    "# gaid:43d2023e-60f0-4459-ac6b-d4fe45e03d9f;1739705835:0.165162265301,268232120:0.708328306675,2508174941:0.742438793182,3987996473:0.184657230973\n",
    "\n",
    "old_lines_df = show(\n",
    "    df=spark.read.csv(\n",
    "        path=path,\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    ).persist(StorageLevel.MEMORY_ONLY),\n",
    "    message=\"prod\",\n",
    "    heavy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"hdfs:/user/vlk/test/TRG-73014/dm_extended_features_v2/new/\"\n",
    "# path=\"hdfs:/data/dm/prj/dev/apps/export/universal_features/dm_extended_features_v2/2022-01-09/\"\n",
    "# path = \"hdfs:/data/dm/prj/dev/apps/export/universal_features/dm_user_af_shows/2022-02-08/*.csv\"\n",
    "\n",
    "path = \"hdfs:/data/dm/prj/dev/apps/export/universal_features/dm_user_app_cats_pc_cri_wscore/2022-02-10/*.csv\"\n",
    "\n",
    "new_lines_df = show(\n",
    "    df=spark.read.csv(\n",
    "        path=path,\n",
    "        header=True,\n",
    "        sep=\";\"\n",
    "    ).persist(StorageLevel.MEMORY_ONLY),\n",
    "    message=\"dev\",\n",
    "    heavy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug only!\n",
    "\n",
    "# gaid:00079fe7-7e6a-4103-9c2f-cdabe3ab4023\n",
    "# 24.9355,1.0,773,7.5997,0.8192,0.0194,0.0323,0.0323,0.0944,0.0815,0.0388,0.0595,0.0595,0.0957,0.0375,0.0375,0.0543,0.0298,0.0789,0.0246,0.0142,0.0479,0.0285,0.0181,0.0129,0.0712,0.0142,0.0168,0.0\n",
    "# std = 0.8192\n",
    "def make_fake_diff(df):\n",
    "    return (df.selectExpr(\n",
    "        \"user\",\n",
    "        \"`num:mean_shows_per_day`\",\n",
    "        \"`num:visit_probability`\",\n",
    "        \"`num:total_shows_cnt`\",\n",
    "        \"`num:mean`\",\n",
    "        \"`num:hours`\"\n",
    "        )\n",
    "        .where(\"user = 'gaid:00079fe7-7e6a-4103-9c2f-cdabe3ab4023'\")\n",
    "        .withColumn(\"num:std\", sqlfn.lit(\"0.8191\"))\n",
    "    )\n",
    "\n",
    "# new_lines_df = make_fake_diff(new_lines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dm_user_app_cats_pc_cri_wscore_join():\n",
    "    # num:dm_user_app_cats_pc_cri_wscore\n",
    "    return show(\n",
    "        df=old_lines_df.alias(\"a\")\n",
    "        .join(new_lines_df.alias(\"b\"), [\"user\"])\n",
    "        .selectExpr(\n",
    "            \"user\",\n",
    "            \"a.`num:dm_user_app_cats_pc_cri_wscore` as a_f\",\n",
    "            \"b.`num:dm_user_app_cats_pc_cri_wscore` as b_f\"\n",
    "        ).persist(StorageLevel.MEMORY_ONLY),\n",
    "        message=\"joined\",\n",
    "        heavy=False\n",
    "    )\n",
    "\n",
    "joined = _dm_user_app_cats_pc_cri_wscore_join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "joined, rows: 1:\n",
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- a_f: string (nullable = false)\n",
      " |-- b_f: string (nullable = false)\n",
      "\n",
      "+-----------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user                                     |a_f                                                                                                                                                                                               |b_f                                                                                                                                                                                                 |\n",
      "+-----------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|gaid:00079fe7-7e6a-4103-9c2f-cdabe3ab4023|24.9355,1.0,773,7.5997,0.8192,0.0194,0.0323,0.0323,0.0944,0.0815,0.0388,0.0595,0.0595,0.0957,0.0375,0.0375,0.0543,0.0298,0.0789,0.0246,0.0142,0.0479,0.0285,0.0181,0.0129,0.0712,0.0142,0.0168,0.0|24.9355,1.0,773.0,7.5997,0.8191,0.0194,0.0323,0.0323,0.0944,0.0815,0.0388,0.0595,0.0595,0.0957,0.0375,0.0375,0.0543,0.0298,0.0789,0.0246,0.0142,0.0479,0.0285,0.0181,0.0129,0.0712,0.0142,0.0168,0.0|\n",
      "+-----------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _dm_user_af_shows_join():\n",
    "    # set of features, dm_user_af_shows\n",
    "    features = [\n",
    "        \"num:mean_shows_per_day\",\n",
    "        \"num:visit_probability\",\n",
    "        \"num:total_shows_cnt\",\n",
    "        \"num:mean\",\n",
    "        \"num:std\",\n",
    "        \"num:hours\"\n",
    "    ]\n",
    "\n",
    "    return show(\n",
    "        df=old_lines_df.alias(\"a\")\n",
    "        .join(new_lines_df.alias(\"b\"), [\"user\"])\n",
    "        .selectExpr(\n",
    "            \"user\",\n",
    "            \"concat_ws(',', {}) as a_f\".format(\", \".join([\"a.`{}`\".format(x) for x in features])),\n",
    "            \"concat_ws(',', {}) as b_f\".format(\", \".join([\"b.`{}`\".format(x) for x in features]))\n",
    "        ).persist(StorageLevel.MEMORY_ONLY),\n",
    "        message=\"joined\",\n",
    "        heavy=False\n",
    "    )\n",
    "\n",
    "# joined = _dm_user_af_shows_join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually for normalization you should for each key compute (min,max) and then map found ranges to (0,1)\n",
    "def _format_map(item):\n",
    "    k, v = item.split(\":\")    \n",
    "    v = str(int(float(v) * 100000))  # 5 fraction digits for float32\n",
    "    return \"{}:{}\".format(k, v[:6])  # only first 6 digits to compare\n",
    "\n",
    "def _normalize_maps(text):\n",
    "    items = text.split(\",\")\n",
    "    formatted_items = [_format_map(x) for x in items]\n",
    "    return sorted(formatted_items, key=lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "def _diff_maps(a, b):\n",
    "    a_items = _normalize_maps(a)\n",
    "    b_items = _normalize_maps(b)\n",
    "    if a_items == b_items:\n",
    "        return None, None\n",
    "\n",
    "    if len(a_items) != len(b_items):\n",
    "        return -1, 100500.0  # key = -1 if len is different\n",
    "\n",
    "    for i, (ai, bi) in enumerate(zip(a_items, b_items)):\n",
    "        ak, av = ai.split(\":\")\n",
    "        bk, bv = bi.split(\":\")\n",
    "        if ak != bk:\n",
    "            return int(ak), float(bk)  # diff = key(b) if keys is different\n",
    "\n",
    "        diff = abs(int(av) - int(bv))\n",
    "        if diff > 1:\n",
    "            return int(ak), float(diff) / 100000.0  # key(a), abs_diff(val(a), val(b))\n",
    "\n",
    "    return None, None  # if a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually for normalization you should for each key compute (min,max) and then map found ranges to (0,1)\n",
    "def _format_num(item):\n",
    "    # works ok only for numbers shape like 1.23456\n",
    "    # 5 fraction digits for float32 \n",
    "    # only first 6 digits to compare\n",
    "    return str(int(float(item) * 100000))[:6]\n",
    "\n",
    "def _normalize_arrays(text):\n",
    "    return [_format_num(x) for x in text.split(\",\")]\n",
    "\n",
    "def _diff_arrays(a, b):\n",
    "    a_items = _normalize_arrays(a) # array of strings\n",
    "    b_items = _normalize_arrays(b)\n",
    "    if a_items == b_items:\n",
    "        return None, None\n",
    "\n",
    "    if len(a_items) != len(b_items):\n",
    "        return -1, 100500.0  # key = -1 if len is different\n",
    "\n",
    "    for i, (ai, bi) in enumerate(zip(a_items, b_items)):\n",
    "        diff = abs(int(ai) - int(bi))\n",
    "        if diff > 1:\n",
    "            return i, float(diff) / 100000.0  # idx, abs_diff(a[i], b[i])\n",
    "\n",
    "    return None, None  # if a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(rows):\n",
    "    for row in rows:\n",
    "        key, diff = _diff_maps(row[\"a_f\"], row[\"b_f\"])\n",
    "        # key, diff = _diff_arrays(row[\"a_f\"], row[\"b_f\"])\n",
    "        yield row[\"user\"], key, diff, row[\"a_f\"], row[\"b_f\"]\n",
    "\n",
    "diff = spark.createDataFrame(\n",
    "    joined.rdd.mapPartitions(transform, preservesPartitioning=True),\n",
    "    schema=\"user:string,key:int,diff:float,a_f:string,b_f:string\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff, rows: 0:\n",
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      " |-- diff: float (nullable = true)\n",
      " |-- a_f: string (nullable = true)\n",
      " |-- b_f: string (nullable = true)\n",
      "\n",
      "+----+---+----+---+---+\n",
      "|user|key|diff|a_f|b_f|\n",
      "+----+---+----+---+---+\n",
      "+----+---+----+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user: string, key: int, diff: float, a_f: string, b_f: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = \"hdfs:/user/vlk/test/TRG-73014/dm_user_app_cats_pc_cri_wscore/old_new_diff/\"\n",
    "\n",
    "diff.where(\n",
    "    \"diff is not null\"\n",
    ").write.option(\n",
    "    \"mapreduce.fileoutputcommitter.algorithm.version\", \"2\"\n",
    ").parquet(\n",
    "    dir_path, \n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "diff = spark.read.parquet(dir_path).persist(StorageLevel.MEMORY_ONLY)\n",
    "show(diff, \"diff\", heavy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff.unpersist()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
