{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from pprint import pformat\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from operator import and_\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "from pyspark.sql.types import MapType, ArrayType, FloatType, StringType, NumericType\n",
    "\n",
    "if six.PY3:\n",
    "    from functools import reduce  # make flake8 happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(dict(os.environ), width=1)\n",
    "def log(obj, msg=\"\"):\n",
    "    if msg: print(msg)\n",
    "    print(\"type: {}\\ndata: {}\".format(type(obj), pformat(obj, indent=1, width=1)))\n",
    "\n",
    "log(os.environ, \"os.environ\")\n",
    "print()\n",
    "log(dict(os.environ), \"dict(os.environ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Pack executable Grinder conda environment into zip\n",
    "TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "env_name = os.path.basename(env_dir)\n",
    "env_archive = \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)\n",
    "\n",
    "# you need this only first time!\n",
    "# !rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with prj conda environment and JVM extensions\n",
    "# `spark-submit ... --driver-java-options \"-Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\"`\n",
    "# spark.driver.extraJavaOptions\n",
    "\n",
    "spark = (\n",
    "SparkSession.builder\n",
    "    .master(\"yarn-client\")\n",
    "    .appName(\"TRG-73014-test\")\n",
    "    .config(\"spark.yarn.queue\", \"dev.other.regular\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"4G\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2G\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1024\")\n",
    "    .config(\"spark.driver.memory\", \"4G\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\")\n",
    "    .config(\"spark.speculation\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"256\")\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\n",
    "    .config(\"spark.network.timeout\", \"800s\")\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\")\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "    .config(\"hive.exec.max.dynamic.partitions\", \"1000000\")\n",
    "    .config(\"hive.exec.max.dynamic.partitions.pernode\", \"100000\")\n",
    "    .config(\"spark.yarn.dist.archives\", env_archive)\n",
    "    .getOrCreate()\n",
    ")\n",
    "#     .config(\"spark.jars\", \"hdfs:/lib/prj-transformers-assembly-dev-1.5.0.jar\")\n",
    "sql_ctx = SQLContext(spark.sparkContext)\n",
    "(spark, sql_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of env. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pformat\n",
    "\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import DataFrame, SQLContext\n",
    "from pyspark.sql.types import (\n",
    "    MapType, ArrayType, FloatType, DoubleType, StringType, StructType, IntegralType, IntegerType\n",
    ")\n",
    "from pyspark.sql.utils import CapturedException\n",
    "from pyspark.ml.wrapper import JavaWrapper\n",
    "\n",
    "from luigi.contrib.hdfs import HdfsTarget\n",
    "\n",
    "from dmprj.apps.utils.common import add_days\n",
    "from dmprj.apps.utils.common.hive import format_table, select_clause\n",
    "from dmprj.apps.utils.common.luigix import HiveExternalTask\n",
    "from dmprj.apps.utils.control.luigix.task import ControlApp\n",
    "from dmprj.apps.utils.control.client.exception import FailedStatusException, MissingDepsStatusException\n",
    "\n",
    "from dmprj.apps.utils.common import unfreeze_json_param\n",
    "from dmprj.apps.utils.common.fs import HdfsClient\n",
    "from dmprj.apps.utils.common.hive import FindPartitionsEngine\n",
    "from dmprj.apps.utils.common.spark import prjUDFLibrary\n",
    "from dmprj.apps.utils.common.luigix import HiveTableSchemaTarget\n",
    "\n",
    "from dmprj.apps.utils.common.hive import select_clause\n",
    "from dmprj.apps.utils.common.spark import prjUDFLibrary, insert_into_hive\n",
    "from dmprj.apps.utils.common.luigix import HiveExternalTask, HiveGenericTarget\n",
    "from dmprj.apps.utils.control.luigix import ControlApp, ControlDynamicOutputPySparkTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomUDFLibrary(spark, \"hdfs:/lib/prj-transformers-assembly-dev-1.5.0.jar\").register_all_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, message=\"dataframe\", nlines=20, truncate=False):\n",
    "    print(\"\\n{}, rows: {}:\".format(message, df.count()))\n",
    "    df.printSchema()\n",
    "    df.show(nlines, truncate)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataframe, rows: 6:\n",
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- friends_info: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- os_info: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- interests: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n",
      "+---+------------+---------+----------+\n",
      "|uid|friends_info|os_info  |interests |\n",
      "+---+------------+---------+----------+\n",
      "|a  |[1 -> 2.0]  |[1 -> 22]|[1 -> 222]|\n",
      "|b  |[2 -> 3.0]  |[2 -> 33]|[2 -> 333]|\n",
      "|c  |null        |[3 ->]   |[]        |\n",
      "|d  |[]          |null     |[4 ->]    |\n",
      "|e  |[5 ->]      |[]       |null      |\n",
      "|f  |null        |null     |null      |\n",
      "+---+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map<i32,float> friends_info, \n",
    "# map<i32,i64> os_info, \n",
    "# map<i32,i32> interests\n",
    "df = show(spark.createDataFrame(\n",
    "        [\n",
    "            [\"a\", {1:2.0}, {1:22}, {1:222}],\n",
    "            [\"b\", {2:3.0}, {2:33}, {2:333}],\n",
    "            [\"c\", None, {3:None}, {}],\n",
    "            [\"d\", {}, None, {4:None}],\n",
    "            [\"e\", {5:None}, {}, None],\n",
    "            [\"f\", None, None, None],\n",
    "        ],\n",
    "        schema=\"uid:string,friends_info:map<int,float>,os_info:map<int,bigint>,interests:map<int,int>\",\n",
    "    ).persist(StorageLevel.MEMORY_ONLY)\n",
    ")\n",
    "\n",
    "spark.catalog.dropGlobalTempView(\"test_features\")\n",
    "df.createGlobalTempView(\"test_features\")  # global_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments with Spark SQL functions\n",
    "# https://spark.apache.org/docs/latest/api/sql/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataframe, rows: 6:\n",
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- merged: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------+\n",
      "|uid|merged                           |\n",
      "+---+---------------------------------+\n",
      "|a  |[1 -> 2.0, 1 -> 22.0, 1 -> 222.0]|\n",
      "|b  |[2 -> 3.0, 2 -> 33.0, 2 -> 333.0]|\n",
      "|c  |null                             |\n",
      "|d  |null                             |\n",
      "|e  |null                             |\n",
      "|f  |null                             |\n",
      "+---+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, merged: map<int,float>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat (merge) maps\n",
    "expr = \"\"\"\n",
    "map_concat(friends_info, os_info, interests) as merged\n",
    "\"\"\"\n",
    "show(spark.sql(\"select uid, {} from global_temp.test_features\".format(expr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataframe, rows: 6:\n",
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- fi: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      " |-- oi: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- it: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n",
      "+---+----------+------------+-------------+\n",
      "|uid|fi        |oi          |it           |\n",
      "+---+----------+------------+-------------+\n",
      "|a  |[1 -> 2.0]|[1001 -> 22]|[2001 -> 222]|\n",
      "|b  |[2 -> 3.0]|[1002 -> 33]|[2002 -> 333]|\n",
      "|c  |[]        |[1003 ->]   |[]           |\n",
      "|d  |[]        |[]          |[2004 ->]    |\n",
      "|e  |[5 ->]    |[]          |[]           |\n",
      "|f  |[]        |[]          |[]           |\n",
      "+---+----------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, fi: map<int,float>, oi: map<int,bigint>, it: map<int,int>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform maps with key bins\n",
    "# https://github.com/klout/brickhouse/blob/863a370820f64a7825c337f708116149978c097a/src/main/java/brickhouse/udf/collect/MapKeyValuesUDF.java#L72\n",
    "show(df.selectExpr(\"uid\", \"\"\"\n",
    "map_from_entries(\n",
    "  transform(\n",
    "    user_dmdesc.map_key_values(coalesce(friends_info, from_json('{}', 'map<int,float>'))), _tup -> (_tup['key'], _tup['value'])\n",
    "  )\n",
    ") as fi\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "map_from_entries(\n",
    "  transform(\n",
    "    user_dmdesc.map_key_values(coalesce(os_info, from_json('{}', 'map<int,long>'))), _tup -> (_tup['key'] + 1000, _tup['value'])\n",
    "  )\n",
    ") as oi\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "map_from_entries(\n",
    "  transform(\n",
    "    user_dmdesc.map_key_values(coalesce(interests, from_json('{}', 'map<int,int>'))), _tup -> (_tup['key'] + 2000, _tup['value'])\n",
    "  )\n",
    ") as it\n",
    "\"\"\"\n",
    "))\n",
    "# map<i32,float> friends_info, \n",
    "# map<i32,i64> os_info, \n",
    "# map<i32,i32> interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataframe, rows: 6:\n",
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- merged: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------+\n",
      "|uid|merged                                 |\n",
      "+---+---------------------------------------+\n",
      "|a  |[1 -> 2.0, 1001 -> 22.0, 2001 -> 222.0]|\n",
      "|b  |[2 -> 3.0, 1002 -> 33.0, 2002 -> 333.0]|\n",
      "|c  |[1003 ->]                              |\n",
      "|d  |[2004 ->]                              |\n",
      "|e  |[5 ->]                                 |\n",
      "|f  |[]                                     |\n",
      "+---+---------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, merged: map<int,float>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform and merge maps\n",
    "show(df.selectExpr(\"uid\",\n",
    "\"\"\"\n",
    "map_concat(\n",
    "        map_from_entries(transform(\n",
    "            user_dmdesc.map_key_values(coalesce(friends_info, from_json('{}', 'map<int,float>'))), _tup -> (_tup['key'], _tup['value'])\n",
    "        )), \n",
    "        map_from_entries(transform(\n",
    "            user_dmdesc.map_key_values(coalesce(os_info, from_json('{}', 'map<int,long>'))), _tup -> (_tup['key'] + 1000, _tup['value'])\n",
    "          )), \n",
    "        map_from_entries(transform(\n",
    "            user_dmdesc.map_key_values(coalesce(interests, from_json('{}', 'map<int,int>'))), _tup -> (_tup['key'] + 2000, _tup['value'])\n",
    "          ))\n",
    ") as merged\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrcol: array (nullable = false)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n",
      "+----------+\n",
      "|arrcol    |\n",
      "+----------+\n",
      "|[1.0, 4.0]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter not null array values\n",
    "df_0 = show(spark.sql(\n",
    "    \"select filter(\"\n",
    "    \"array(\"\n",
    "    \"cast(1 as float), cast(null as float), cast('NaN' as float), cast(4 as float)\"\n",
    "    \"), _x -> not isnull(_x) and not isnan(_x)\"\n",
    "    \") as arrcol\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mapcol: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: double (valueContainsNull = true)\n",
      "\n",
      "+----------------------------------------------------------------+\n",
      "|mapcol                                                          |\n",
      "+----------------------------------------------------------------+\n",
      "|[a -> 1.0, b ->, c -> NaN, 101 -> 1.01, -1 -> 2.02, 303 -> 3.03]|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create map from two arrays\n",
    "df_1 = show(\n",
    "    spark.sql(\n",
    "        \"select map_from_arrays(\"\n",
    "        \"array('a', 'b', 'c', '101', '-1', '303'), \"\n",
    "        \"array(cast(1 as double), cast(null as double), cast('NaN' as double), 1.01, 2.02, 3.03)\"\n",
    "        \") as mapcol\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrtuples: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: double (nullable = true)\n",
      "\n",
      "+----------------------------------------------------------------+\n",
      "|arrtuples                                                       |\n",
      "+----------------------------------------------------------------+\n",
      "|[[a, 1.0], [b,], [101, 1.01], [c, NaN], [-1, 2.02], [303, 3.03]]|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert map to array\n",
    "df_2 = show(df_1.selectExpr(\n",
    "    \"user_dmdesc.map_key_values(mapcol) as arrtuples\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrtuples: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: double (nullable = true)\n",
      "\n",
      "+--------------------------+\n",
      "|arrtuples                 |\n",
      "+--------------------------+\n",
      "|[[101, 1.01], [303, 3.03]]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop invalid tuples\n",
    "df_3 = show(df_2.selectExpr(\n",
    "    \"filter(arrtuples, _x -> \"\n",
    "    \"not isnull(_x['value']) and not isnan(_x['value']) and is_uint32(_x['key'])\"\n",
    "    \") as arrtuples\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mapcol: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      "\n",
      "+--------------------------+\n",
      "|mapcol                    |\n",
      "+--------------------------+\n",
      "|[101 -> 1.01, 303 -> 3.03]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert array of tuples to map\n",
    "df_4 = show(df_3.selectExpr(\n",
    "    \"cast(\"\n",
    "    \"map_from_entries(arrtuples)\"\n",
    "    \"as map<string,float>) as mapcol\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataframe, rows: 1:\n",
      "root\n",
      " |-- entries: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      "\n",
      "+-------+\n",
      "|entries|\n",
      "+-------+\n",
      "|[]     |\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[entries: map<int,float>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# js = '{\"1\":2}'\n",
    "js = '{}'\n",
    "show(spark.sql(\"select from_json('{}', 'map<int,float>')\".format(js)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|CAST(0 AS FLOAT)|\n",
      "+----------------+\n",
      "|             0.0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cast(0 as float)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, friends_info: map<int,float>, os_info: map<int,bigint>, interests: map<int,int>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropGlobalTempView(\"test_features\")\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
