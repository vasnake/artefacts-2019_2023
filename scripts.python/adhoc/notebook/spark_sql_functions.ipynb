{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from pprint import pformat\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from operator import and_\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "from pyspark.sql.types import MapType, ArrayType, FloatType, StringType, NumericType\n",
    "\n",
    "if six.PY3:\n",
    "    from functools import reduce  # make flake8 happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(dict(os.environ), width=1)\n",
    "def log(obj, msg=\"\"):\n",
    "    if msg: print(msg)\n",
    "    print(\"type: {}\\ndata: {}\".format(type(obj), pformat(obj, indent=1, width=1)))\n",
    "\n",
    "log(os.environ, \"os.environ\")\n",
    "print()\n",
    "log(dict(os.environ), \"dict(os.environ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Pack executable Grinder conda environment into zip\n",
    "TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "env_name = os.path.basename(env_dir)\n",
    "env_archive = \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)\n",
    "\n",
    "# you need this only first time!\n",
    "# !rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with prj conda environment and JVM extensions\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"yarn-client\")\\\n",
    "    .appName(\"dmprj-spark_sql_functions_test\")\\\n",
    "    .config(\"spark.yarn.queue\", \"dev.regular\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"2G\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"1G\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"128\")\\\n",
    "    .config(\"spark.network.timeout\", \"800s\")\\\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\\\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\\\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\\\n",
    "    .config(\"spark.driver.memory\", \"2G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "    .config(\"hive.exec.max.dynamic.partitions\", \"100000\")\\\n",
    "    .config(\"hive.exec.max.dynamic.partitions.pernode\", \"10000\")\\\n",
    "    .config(\"spark.jars\", \"hdfs:/lib/prj-transformers-assembly-1.4.0.jar\")\\\n",
    "    .config(\"spark.yarn.dist.archives\", env_archive)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sql_ctx = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# end of env. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pprint import pformat\n",
    "\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "from pyspark.sql.types import MapType, ArrayType, FloatType, DoubleType, StringType, StructType, IntegralType\n",
    "from pyspark.sql.utils import CapturedException\n",
    "\n",
    "from luigi.contrib.hdfs import HdfsTarget\n",
    "\n",
    "from dmprj.apps.utils.common import add_days\n",
    "from dmprj.apps.utils.common.hive import format_table, select_clause\n",
    "from dmprj.apps.utils.common.luigix import HiveExternalTask\n",
    "from dmprj.apps.utils.control.luigix.task import ControlApp\n",
    "from dmprj.apps.utils.control.client.exception import FailedStatusException, MissingDepsStatusException\n",
    "\n",
    "from dmprj.apps.utils.common import unfreeze_json_param\n",
    "from dmprj.apps.utils.common.fs import HdfsClient\n",
    "from dmprj.apps.utils.common.hive import FindPartitionsEngine\n",
    "from dmprj.apps.utils.common.spark import prjUDFLibrary\n",
    "from dmprj.apps.utils.common.luigix import HiveTableSchemaTarget\n",
    "\n",
    "from dmprj.apps.utils.common.hive import select_clause\n",
    "from dmprj.apps.utils.common.spark import prjUDFLibrary, insert_into_hive\n",
    "from dmprj.apps.utils.common.luigix import HiveExternalTask, HiveGenericTarget\n",
    "from dmprj.apps.utils.control.luigix import ControlApp, ControlDynamicOutputPySparkTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomUDFLibrary(spark).register_all_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, nlines=10, truncate=False):\n",
    "    df.printSchema()\n",
    "    df.show(nlines, truncate)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments with Spark SQL functions\n",
    "# https://spark.apache.org/docs/latest/api/sql/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrcol: array (nullable = false)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n",
      "+----------+\n",
      "|arrcol    |\n",
      "+----------+\n",
      "|[1.0, 4.0]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter array values\n",
    "df_0 = show(spark.sql(\n",
    "    \"select filter(\"\n",
    "    \"array(\"\n",
    "    \"cast(1 as float), cast(null as float), cast('NaN' as float), cast(4 as float)\"\n",
    "    \"), _x -> not isnull(_x) and not isnan(_x)\"\n",
    "    \") as arrcol\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mapcol: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: double (valueContainsNull = true)\n",
      "\n",
      "+----------------------------------------------------------------+\n",
      "|mapcol                                                          |\n",
      "+----------------------------------------------------------------+\n",
      "|[a -> 1.0, b ->, c -> NaN, 101 -> 1.01, -1 -> 2.02, 303 -> 3.03]|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create map\n",
    "df_1 = show(\n",
    "    spark.sql(\n",
    "        \"select map_from_arrays(\"\n",
    "        \"array('a', 'b', 'c', '101', '-1', '303'), \"\n",
    "        \"array(cast(1 as double), cast(null as double), cast('NaN' as double), 1.01, 2.02, 3.03)\"\n",
    "        \") as mapcol\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrtuples: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: double (nullable = true)\n",
      "\n",
      "+----------------------------------------------------------------+\n",
      "|arrtuples                                                       |\n",
      "+----------------------------------------------------------------+\n",
      "|[[a, 1.0], [b,], [101, 1.01], [c, NaN], [-1, 2.02], [303, 3.03]]|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert map to array\n",
    "df_2 = show(df_1.selectExpr(\n",
    "    \"user_dmdesc.map_key_values(mapcol) as arrtuples\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrtuples: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: double (nullable = true)\n",
      "\n",
      "+--------------------------+\n",
      "|arrtuples                 |\n",
      "+--------------------------+\n",
      "|[[101, 1.01], [303, 3.03]]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop invalid tuples\n",
    "df_3 = show(df_2.selectExpr(\n",
    "    \"filter(arrtuples, _x -> \"\n",
    "    \"not isnull(_x['value']) and not isnan(_x['value']) and is_uint32(_x['key'])\"\n",
    "    \") as arrtuples\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mapcol: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: float (valueContainsNull = true)\n",
      "\n",
      "+--------------------------+\n",
      "|mapcol                    |\n",
      "+--------------------------+\n",
      "|[101 -> 1.01, 303 -> 3.03]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert array to map\n",
    "df_4 = show(df_3.selectExpr(\n",
    "    \"cast(\"\n",
    "    \"map_from_entries(arrtuples)\"\n",
    "    \"as map<string,float>) as mapcol\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|CAST(0 AS FLOAT)|\n",
      "+----------------+\n",
      "|             0.0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cast(0 as float)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
