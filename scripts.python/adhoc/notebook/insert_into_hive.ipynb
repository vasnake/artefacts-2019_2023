{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert_into_hive experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from pprint import pformat\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from operator import and_\n",
    "from collections import defaultdict\n",
    "\n",
    "import six\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "from pyspark.sql.types import MapType, ArrayType, FloatType, StringType, NumericType\n",
    "\n",
    "if six.PY3:\n",
    "    from functools import reduce  # make flake8 happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(dict(os.environ), width=1)\n",
    "def log(obj, msg=\"\"):\n",
    "    if msg: print(msg)\n",
    "    print(\"type: {}\\ndata: {}\".format(type(obj), pformat(obj, indent=1, width=1)))\n",
    "\n",
    "log(os.environ, \"os.environ\")\n",
    "print()\n",
    "log(dict(os.environ), \"dict(os.environ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Pack executable prj conda environment into zip\n",
    "TMP_ENV_BASEDIR = \"tmpenv\"  # Reserved directory to store environment archive\n",
    "env_dir = os.path.dirname(os.path.dirname(sys.executable))\n",
    "env_name = os.path.basename(env_dir)\n",
    "env_archive = \"{basedir}/{env}.zip#{basedir}\".format(basedir=TMP_ENV_BASEDIR, env=env_name)\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"{}/{}/bin/python\".format(TMP_ENV_BASEDIR, env_name)\n",
    "\n",
    "# you need this only first time!\n",
    "# !rm -rf {TMP_ENV_BASEDIR} && mkdir {TMP_ENV_BASEDIR} && cd {TMP_ENV_BASEDIR} && rsync -a {env_dir} . && zip -rq {env_name}.zip {env_name}\n",
    "\n",
    "log(env_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with prj conda environment and JVM extensions\n",
    "# `spark-submit ... --driver-java-options \"-Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\"`\n",
    "# spark.driver.extraJavaOptions\n",
    "queue = \"root.regular\"\n",
    "\n",
    "# \"spark.driver.extraJavaOptions\", \"-Xss10M\"\n",
    "# catalyst SO while building parts. filter expression\n",
    "\n",
    "# 4 TB of data\n",
    "# sssp = 4 * 4 * 1024\n",
    "sssp = 1 * 4 * 1024\n",
    "\n",
    "spark = (\n",
    "SparkSession.builder\n",
    "    .master(\"yarn-client\")\n",
    "    .appName(\"TRG-75523-insert_into_hive-test-ipynb\")\n",
    "    .config(\"spark.yarn.queue\", queue)\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"8G\")\n",
    "    .config(\"spark.executor.cores\", \"6\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2G\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", sssp)\n",
    "    .config(\"spark.driver.memory\", \"4G\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/home/vlk/driver2_log4j.properties\")\n",
    "    .config(\"spark.speculation\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"4\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"512\")\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\n",
    "    .config(\"spark.network.timeout\", \"800s\")\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"10\")\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"60s\")\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition\", \"true\")\n",
    "    .config(\"spark.hadoop.hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions\", \"1000000\")\n",
    "    .config(\"spark.hadoop.hive.exec.max.dynamic.partitions.pernode\", \"100000\")\n",
    "    .config(\"spark.hadoop.hive.metastore.client.socket.timeout\", \"3600s\")\n",
    "    .config(\"spark.ui.enabled\", \"true\")\n",
    "    .config(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "    .config(\"spark.yarn.dist.archives\", env_archive)\n",
    "    .getOrCreate()\n",
    ")\n",
    "# .config(\"spark.driver.extraJavaOptions\", \"-Xss10M -Dlog4j.configuration=file:/home/vlk/driver_log4j.properties\")\n",
    "#     .config(\"spark.jars\", \"hdfs:/lib/prj-transformers-assembly-dev-1.5.1.jar\")\n",
    "sql_ctx = SQLContext(spark.sparkContext)\n",
    "(spark, sql_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of env. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pformat\n",
    "\n",
    "import luigi\n",
    "import pyspark.sql.functions as sqlfn\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import DataFrame, SQLContext\n",
    "from pyspark.sql.types import (\n",
    "    MapType, ArrayType, FloatType, DoubleType, StringType, StructType, IntegralType, IntegerType\n",
    ")\n",
    "from pyspark.sql.utils import CapturedException\n",
    "from pyspark.ml.wrapper import JavaWrapper\n",
    "\n",
    "from luigi.contrib.hdfs import HdfsTarget\n",
    "\n",
    "from dmprj.apps.utils.common import add_days\n",
    "from dmprj.apps.utils.common.hive import format_table, select_clause\n",
    "from dmprj.apps.utils.common.luigix import HiveExternalTask\n",
    "from dmprj.apps.utils.control.luigix.task import ControlApp\n",
    "from dmprj.apps.utils.control.client.exception import FailedStatusException, MissingDepsStatusException\n",
    "\n",
    "from dmprj.apps.utils.common import unfreeze_json_param\n",
    "from dmprj.apps.utils.common.fs import HdfsClient\n",
    "from dmprj.apps.utils.common.hive import FindPartitionsEngine\n",
    "from dmprj.apps.utils.common.spark import prjUDFLibrary\n",
    "from dmprj.apps.utils.common.luigix import HiveTableSchemaTarget\n",
    "\n",
    "from dmprj.apps.utils.common.hive import select_clause\n",
    "from dmprj.apps.utils.common.spark import prjUDFLibrary, insert_into_hive\n",
    "from dmprj.apps.utils.common.luigix import HiveExternalTask, HiveGenericTarget\n",
    "from dmprj.apps.utils.control.luigix import ControlApp, ControlDynamicOutputPySparkTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomUDFLibrary(spark, \"hdfs:/lib/prj-transformers-assembly-dev-1.5.1.jar\").register_all_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, message=\"dataframe\", nlines=20, truncate=False, heavy=True):\n",
    "    if not heavy: print(\"\\n{}, rows: {}:\".format(message, df.count()))\n",
    "    df.printSchema()\n",
    "    if not heavy: df.show(nlines, truncate)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reload table to `dt=yyyy-MM-dd` partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload():\n",
    "    temp_path = \"/user/vlk/test/ds_auditories/mob_app_audience\"\n",
    "    df = spark.read.parquet(os.path.join(temp_path, \"source\")).persist()\n",
    "    (\n",
    "        df.coalesce(1024 * 8)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .partitionBy(\"dt\")\n",
    "        .parquet(os.path.join(temp_path, \"parts\"))\n",
    "    )\n",
    "    df.unpersist()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ds_auditories.mob_app_audience tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_sql = \"drop table if exists user_vlk.mob_app_audience_TRG75523 purge\"\n",
    "\n",
    "# TODO: check if this true: for some reason, spark create table is uncompatible with rest of the ETL pipeline.\n",
    "# creating table in hive console will do nicely.\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE user_vlk.mob_app_audience_TRG75523 (\n",
    "  `uid` string COMMENT 'Uid',\n",
    "  `score` double COMMENT 'Uid [0,1]-interval app install duration ranking score')\n",
    "COMMENT 'Installed mobile apps user lists in audience format'\n",
    "PARTITIONED BY (\n",
    "  `audience_name` string COMMENT 'mobile app store id as in store_id from md_mobile.mobile_app',\n",
    "  `category` string COMMENT 'mobile app aggregation type, positive stands for default last year aggregation period',\n",
    "  `dt` string COMMENT 'Data date',\n",
    "  `uid_type` string COMMENT 'Type of uid, e.g. GAID, IDFA')\n",
    "ROW FORMAT SERDE\n",
    "  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\n",
    "STORED AS INPUTFORMAT\n",
    "  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\n",
    "OUTPUTFORMAT\n",
    "  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(drop_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"REFRESH TABLE ds_auditories.mob_app_audience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from table\n",
    "\n",
    "# (2022-01-07,11972) # corrupted\n",
    "# (2022-01-10,21558)\n",
    "# (2022-01-01,31263) # good\n",
    "# (2022-01-16,32050) # good\n",
    "# (2022-01-17,32101) # good\n",
    "\n",
    "# dts = \"dt in ('2022-01-01', '2022-01-16', '2022-01-17')\"\n",
    "dts = \"dt in ('2022-01-01')\"\n",
    "\n",
    "df = _show(\n",
    "    spark.table(\"ds_auditories.mob_app_audience\")\n",
    "      .where(dts)\n",
    "    .cache(),\n",
    "    \"ds_auditories.mob_app_audience, 1 dt\",\n",
    "    heavy=True\n",
    ")\n",
    "\n",
    "temp_path = \"/user/vlk/test/ds_auditories/mob_app_audience/source\"\n",
    "# df.write.mode(\"overwrite\").parquet(temp_path)\n",
    "# df.unpersist()\n",
    "# df = spark.read.parquet(temp_path).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- audience_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- uid_type: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: string, score: double, audience_name: string, category: string, uid_type: string, dt: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from hdfs\n",
    "\n",
    "base_path = \"/user/vlk/test/ds_auditories/mob_app_audience/parts\"\n",
    "temp_path = base_path\n",
    "\n",
    "# dts = \"dt in ('2022-01-17', '2022-01-16', '2022-01-01')\"\n",
    "dts = \"dt in ('2022-01-17')\"\n",
    "\n",
    "df = (\n",
    "    spark.read.option(\"basePath\", base_path)\n",
    "    .parquet(temp_path)\n",
    "    .where(dts)    \n",
    "    .persist()\n",
    ")\n",
    "# .drop(\"dt\").withColumn(\"dt\", sqlfn.lit(\"2022-01-17\")) \n",
    "# spark.sql.sources.partitionColumnTypeInference.enabled\n",
    "show(df, \"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_hive(\n",
    "    df,\n",
    "    database=\"user_vlk\",\n",
    "    table=\"mob_app_audience_TRG75523\",\n",
    "    max_rows_per_bucket=2505505,\n",
    "    overwrite=True,\n",
    "    raise_on_missing_columns=True,\n",
    "    check_parameter=\"markedAsDataLoaded\",\n",
    "    jar=\"hdfs:/lib/prj-transformers-assembly-dev-1.5.2.jar\"\n",
    ")\n",
    "\n",
    "# 22/01/19 17:14:11 INFO Writer: Updating Hive table partitions parameters `markedAsDataLoaded -> None` for 32101 partitions ...\n",
    "# java.lang.StackOverflowError\n",
    "#\tat org.apache.spark.sql.catalyst.expressions.BinaryOperator.sql(Expression.scala:592)\n",
    "# \"spark.driver.extraJavaOptions\": \"-Xss10M\"\n",
    "\n",
    "# java.lang.StackOverflowError\n",
    "#\tat org.apache.spark.sql.hive.client.Shim_v0_13.org$apache$spark$sql$hive$client$Shim_v0_13$$convert$1(HiveShim.scala:714)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: rewrite one dt, two dt, see how result changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unpersist()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## failed insert_into_hive, exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /user/vlk/test/target_novlk/target\n",
    "df = spark.read.parquet(\"/user/vlk/test/target_novlk/target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dmprj_source.app_feature_extended\n",
    "# show create table dmprj_source.app_feature_extended;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sql = \"\"\"\n",
    "CREATE TABLE `dmprj_source.app_feature_extended`(\n",
    "  `store_id` string COMMENT 'Public store app_id', \n",
    "  `app_type` map<string,int> COMMENT 'Mobile app type: game/application/album', \n",
    "  `developer` map<string,int> COMMENT 'Mobile app developer', \n",
    "  `os_ver` map<string,int> COMMENT 'OS concatenated with version, e.g. iOS_11.0', \n",
    "  `all_categories` map<string,int> COMMENT 'All app categories from App Store or Google Play', \n",
    "  `categories` map<string,int> COMMENT 'Main app category from App Store or Google Play', \n",
    "  `age_limit` map<string,int> COMMENT 'Type of age restriction for mobile app, like 16+ etc.', \n",
    "  `desc_language` map<string,float> COMMENT 'App description language', \n",
    "  `app_topics100` map<string,float> COMMENT 'LDA topics based on mobile app description', \n",
    "  `desc_taxons` map<string,float> COMMENT 'App description taxons assigned by the trg.text.cls.bert.BertCategoryModel', \n",
    "  `desc_embedding` map<string,int> COMMENT 'App description embedding acquired by the trg.encoding.SparseAutoEncoder', \n",
    "  `events` map<string,float> COMMENT 'Scale of number of events, associated with the app, splitted by event type', \n",
    "  `misc_info` array<float> COMMENT 'The rest dense features in order: price, rating, votes_scale, weeks from creation')\n",
    "COMMENT 'Mobile application extended features'\n",
    "PARTITIONED BY ( \n",
    "  `dt` string COMMENT 'Data date', \n",
    "  `os` string COMMENT 'iOS/Android')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_script = \"\"\"\n",
    "drop table if exists dmprj_dev_source.TRG_73352_test purge;\n",
    "\n",
    "create table dmprj_dev_source.TRG_73352_test (\n",
    "  `store_id` string COMMENT 'Public store app_id', \n",
    "  `app_type` map<string,int> COMMENT 'Mobile app type: game/application/album', \n",
    "  `developer` map<string,int> COMMENT 'Mobile app developer', \n",
    "  `os_ver` map<string,int> COMMENT 'OS concatenated with version, e.g. iOS_11.0', \n",
    "  `all_categories` map<string,int> COMMENT 'All app categories from App Store or Google Play', \n",
    "  `categories` map<string,int> COMMENT 'Main app category from App Store or Google Play', \n",
    "  `age_limit` map<string,int> COMMENT 'Type of age restriction for mobile app, like 16+ etc.', \n",
    "  `desc_language` map<string,float> COMMENT 'App description language', \n",
    "  `app_topics100` map<string,float> COMMENT 'LDA topics based on mobile app description', \n",
    "  `desc_taxons` map<string,float> COMMENT 'App description taxons assigned by the trg.text.cls.bert.BertCategoryModel', \n",
    "  `desc_embedding` map<string,int> COMMENT 'App description embedding acquired by the trg.encoding.SparseAutoEncoder', \n",
    "  `events` map<string,float> COMMENT 'Scale of number of events, associated with the app, splitted by event type', \n",
    "  `misc_info` array<float> COMMENT 'The rest dense features in order: price, rating, votes_scale, weeks from creation'\n",
    " )\n",
    "COMMENT 'Mobile application extended features'\n",
    "partitioned by (\n",
    "  `dt` string COMMENT 'Data date', \n",
    "  `os` string COMMENT 'iOS/Android'\n",
    ")\n",
    "stored as orc;\n",
    "\n",
    "gdfs ls -lah /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, **mapping):\n",
    "    show(df)\n",
    "    return show(df.selectExpr(*[\"{} as {}\".format(col, mapping.get(col, col)) for col in df.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition_buckets(df, max_rows_per_bucket, *partition_columns):\n",
    "    log(partition_columns, \"\\npartition columns\")\n",
    "    if not partition_columns:\n",
    "        num_buckets = int(np.ceil(df.count() / float(max_rows_per_bucket)))\n",
    "        return df.repartition(num_buckets)\n",
    "\n",
    "    prefixed_cols = [(\"a_\" + col) for col in df.columns]\n",
    "    prefixed_partition_cols = [(\"a_\" + col) for col in partition_columns]\n",
    "    add_prefix_map = dict(zip(df.columns, prefixed_cols))\n",
    "    remove_prefix_map = dict(zip(prefixed_cols, df.columns))\n",
    "\n",
    "    df = rename_columns(df, **add_prefix_map)\n",
    "    show(df)\n",
    "\n",
    "    part_df = df.groupBy(*prefixed_partition_cols).count()\n",
    "    show(part_df)\n",
    "    part_df = part_df.toPandas()\n",
    "    part_df[\"num_buckets\"] = np.ceil(part_df[\"count\"] / float(max_rows_per_bucket)).astype(int)\n",
    "    part_df[\"beg\"] = part_df[\"num_buckets\"].cumsum() - part_df[\"num_buckets\"]\n",
    "    log(part_df, \"\\npartitions counts\")\n",
    "\n",
    "    partition_map = {}\n",
    "    for _, row in part_df.iterrows():\n",
    "        partition_map.update({tuple(row[prefixed_partition_cols]): (row.beg, row.num_buckets)})\n",
    "    log(partition_map, \"\\nrepartition data\")\n",
    "    partition_map_bc = df.sql_ctx.sparkSession.sparkContext.broadcast(partition_map)\n",
    "\n",
    "    @sqlfn.udf(returnType=IntegerType())\n",
    "    def _partition_index(*cols):\n",
    "        beg, count = partition_map_bc.value[tuple(cols)]\n",
    "        return int(beg + np.random.randint(count))\n",
    "\n",
    "    indexed_df = show(df.withColumn(\"index\", _partition_index(*prefixed_partition_cols)).cache())    \n",
    "    bucket_df = show(\n",
    "        indexed_df.repartitionByRange(\n",
    "            max(part_df[\"num_buckets\"].sum(), 1),\n",
    "            \"index\"\n",
    "        )\n",
    "    ).drop(\"index\")\n",
    "\n",
    "    return rename_columns(bucket_df, **remove_prefix_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_hive(df, database, table, max_rows_per_bucket, overwrite=True, raise_on_missing_columns=True):\n",
    "    spark = df.sql_ctx.sparkSession\n",
    "    columns = spark.catalog.listColumns(dbName=database, tableName=table)\n",
    "    log(columns, \"\\ncatalog columns\")\n",
    "\n",
    "    if not raise_on_missing_columns:\n",
    "        df = df.select(\n",
    "            *[\n",
    "                sqlfn.col(column.name) if column.name in df.columns else sqlfn.lit(None).alias(column.name)\n",
    "                for column in columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    main_columns, partition_columns = [], []\n",
    "\n",
    "    for column in columns:\n",
    "        if column.isPartition:\n",
    "            partition_columns.append(column.name)\n",
    "        else:\n",
    "            main_columns.append(column.name)\n",
    "\n",
    "    old_mode = spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    log(old_mode, \"\\nsaved spark.sql.sources.partitionOverwriteMode\")\n",
    "\n",
    "    try:\n",
    "        create_partition_buckets(\n",
    "            df.select(*(main_columns + partition_columns)), max_rows_per_bucket, *partition_columns\n",
    "        ).write.insertInto(\"{}.{}\".format(database, table), overwrite=overwrite)\n",
    "    finally:\n",
    "        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", old_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"dmprj_dev_source\"\n",
    "table = \"TRG_73352_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert_into_hive(df, database=db, table=table, max_rows_per_bucket=300000, raise_on_missing_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmprj.apps.utils.common.spark import insert_into_hive\n",
    "df = spark.read.parquet(\"/user/vlk/test/target_novlk/target\")\n",
    "insert_into_hive(\n",
    "    df, database=db, table=table, max_rows_per_bucket=1000000, overwrite=True, raise_on_missing_columns=True\n",
    ")\n",
    "\n",
    "t = \"\"\"\n",
    "vlk@host> gdfs du -h /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/\n",
    "9.8M    /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/dt=2021-12-12/os=Android/part-00000-44369aa8-30e8-4a36-91e6-b2d301dc0243.c000\n",
    "9.8M    /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/dt=2021-12-12/os=Android\n",
    "2.5M    /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/dt=2021-12-12/os=iOS/part-00001-44369aa8-30e8-4a36-91e6-b2d301dc0243.c000\n",
    "2.5M    /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/dt=2021-12-12/os=iOS\n",
    "12.3M   /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/dt=2021-12-12\n",
    "12.3M   /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read_orc_table test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_orc_table(what, partition_filter_expr, spark, jar=\"hdfs:/lib/dwh/common-1.21.21.jar\"):\n",
    "    spark.sql(\"ADD JAR {}\".format(jar))\n",
    "    _rtable = spark._sc._jvm.ru.mail.dwh.common.TableUtils.readTableAsUnionOrcFiles\n",
    "    _jdf = _rtable(what, sqlfn.expr(partition_filter_expr)._jc, spark._jsparkSession)\n",
    "    return DataFrame(_jdf, SQLContext(spark.sparkContext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data/dm/prj/dev/hive/dmprj_dev_source.db/bits_user_vector/\n",
    "# source_name=DM-8225-hid-v1/dt=2020-10-16/uid_type=HID\n",
    "df = read_orc_table(\n",
    "    what=\"dmprj_dev_source.bits_user_vector\", \n",
    "    partition_filter_expr=\"source_name='DM-8225-hid-v1' and dt='2020-10-16' and uid_type='HID'\",\n",
    "    spark=spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df, nlines=10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_orc_table_jvm(what, partition_filter_expr, spark, jar=\"hdfs:/lib/dwh/common-1.21.21.jar\"):\n",
    "    from pyspark.ml.wrapper import JavaWrapper\n",
    "    spark.sql(\"ADD JAR {}\".format(jar))\n",
    "\n",
    "    jdf = JavaWrapper._new_java_obj(\n",
    "        \"TableUtils.readTableAsUnionOrcFiles\",\n",
    "        what, sqlfn.expr(partition_filter_expr)._jc, spark._jsparkSession\n",
    "    )\n",
    "    return DataFrame(jdf, SQLContext(spark.sparkContext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data/dm/prj/dev/hive/dmprj_dev_source.db/bits_user_vector/\n",
    "# source_name=DM-8225-hid-v1/dt=2020-10-16/uid_type=HID\n",
    "df = read_orc_table_jvm(\n",
    "    what=\"dmprj_dev_source.bits_user_vector\", \n",
    "    partition_filter_expr=\"source_name='DM-8225-hid-v1' and dt='2020-10-16' and uid_type='HID'\",\n",
    "    spark=spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df, nlines=10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert_into_hive, ~1TB to write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from ds_auditories.mob_app_audience\").where(\"dt = '2021-12-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df, \"input DF\")\n",
    "# 41 059 901 473 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_script = \"\"\"\n",
    "drop table if exists dmprj_dev_source.TRG_73352_test purge;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dmprj_dev_source.TRG_73352_test (\n",
    "  uid           string             comment 'user id',\n",
    "  score         double             comment 'score value'\n",
    ") comment 'user score'\n",
    "partitioned by (\n",
    "  category      string             comment 'category',\n",
    "  dt            string             comment 'date as string',\n",
    "  uid_type      string             comment 'uid type'\n",
    ")\n",
    "stored as orc;\n",
    "\n",
    "gdfs ls -lah /data/dm/prj/dev/hive/dmprj_dev_source.db/trg_73352_test/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS dmprj_dev_source.trg_73352_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dmprj_dev_source.TRG_73352_test (\n",
    "  uid           string             comment 'user id',\n",
    "  score         double             comment 'score value'\n",
    ") comment 'user score'\n",
    "partitioned by (\n",
    "  category      string             comment 'category',\n",
    "  dt            string             comment 'date as string',\n",
    "  uid_type      string             comment 'uid type'\n",
    ")\n",
    "stored as orc\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_hive_jvm(\n",
    "    df,\n",
    "    database,\n",
    "    table,\n",
    "    max_rows_per_bucket,\n",
    "    overwrite=True,\n",
    "    raise_on_missing_columns=True,\n",
    "    check_parameter=None,\n",
    "    jar=\"hdfs:/lib/dm/prj-transformers-assembly-dev-1.5.0.jar\",\n",
    "):\n",
    "    df.sql_ctx.sql(\"ADD JAR {}\".format(jar))\n",
    "    writer = JavaWrapper._create_from_java_class(\"prj.hive.Writer\")  # SQL\n",
    "#     writer = JavaWrapper._create_from_java_class(\"prj.hive.Writer\", \"RDD\")\n",
    "\n",
    "    writer._java_obj.insertIntoHive(\n",
    "        df._jdf,\n",
    "        database,\n",
    "        table,\n",
    "        max_rows_per_bucket,\n",
    "        overwrite,\n",
    "        raise_on_missing_columns,\n",
    "        check_parameter,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_hive_jvm(df, \"dmprj_dev_source\", \"trg_73352_test\", 6000000, True, False, \"markedAsDataLoaded\")\n",
    "# 30 min SQL; 45 min Python; 45 min RDD; 35 min TRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_finder = FindPartitionsEngine()\n",
    "partitions = partitions_finder.find(\n",
    "            database=\"dmprj_dev_source\",\n",
    "            table=\"trg_73352_test\",\n",
    "            partition_conf={\"dt\": \"2021-12-01\"},\n",
    "            min_dt=\"2021-12-01\",\n",
    "            max_dt=\"2021-12-01\",\n",
    "            check_parameter=\"markedAsDataLoaded\",\n",
    ")\n",
    "log((len(partitions), partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmprj.apps.utils.common.spark import CustomUDFLibrary, insert_into_hive\n",
    "insert_into_hive(df, \"dmprj_dev_source\", \"trg_73352_test\", 6000000, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = show(spark.createDataFrame([\n",
    "        # A 09\n",
    "        {\"uid\": \"a\", \"feature\": 1.1, \"uid_type\": \"A\", \"dt\": \"2021-11-09\"},\n",
    "        {\"uid\": \"b\", \"feature\": 1.2, \"uid_type\": \"A\", \"dt\": \"2021-11-09\"},\n",
    "        {\"uid\": \"c\", \"feature\": 1.3, \"uid_type\": \"A\", \"dt\": \"2021-11-09\"},\n",
    "        # A 10\n",
    "        {\"uid\": \"a\", \"feature\": 1.1, \"uid_type\": \"A\", \"dt\": \"2021-11-10\"},\n",
    "        {\"uid\": \"b\", \"feature\": 1.2, \"uid_type\": \"A\", \"dt\": \"2021-11-10\"},\n",
    "        {\"uid\": \"c\", \"feature\": 1.3, \"uid_type\": \"A\", \"dt\": \"2021-11-10\"},\n",
    "        # B 09\n",
    "        {\"uid\": \"a\", \"feature\": 1.1, \"uid_type\": \"B\", \"dt\": \"2021-11-09\"},\n",
    "        {\"uid\": \"b\", \"feature\": 1.2, \"uid_type\": \"B\", \"dt\": \"2021-11-09\"},\n",
    "        {\"uid\": \"c\", \"feature\": 1.3, \"uid_type\": \"B\", \"dt\": \"2021-11-09\"},\n",
    "        # B 10\n",
    "        {\"uid\": \"a\", \"feature\": 1.1, \"uid_type\": \"B\", \"dt\": \"2021-11-10\"},\n",
    "        {\"uid\": \"b\", \"feature\": 1.2, \"uid_type\": \"B\", \"dt\": \"2021-11-10\"},\n",
    "        {\"uid\": \"c\", \"feature\": 1.3, \"uid_type\": \"B\", \"dt\": \"2021-11-10\"},\n",
    "    ]).persist(StorageLevel.MEMORY_ONLY))\n",
    "\n",
    "df.createGlobalTempView(\"test_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"dmprj_dev_source\"\n",
    "table = \"float_feature_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(np.random.randint(7), \"np.random.randint(7), must be 0..6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, **mapping):\n",
    "    show(df)\n",
    "    return show(df.selectExpr(*[\"{} as {}\".format(col, mapping.get(col, col)) for col in df.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition_buckets(df, max_rows_per_bucket, *partition_columns):\n",
    "    log(partition_columns, \"\\npartition columns\")\n",
    "    if not partition_columns:\n",
    "        num_buckets = int(np.ceil(df.count() / float(max_rows_per_bucket)))\n",
    "        return df.repartition(num_buckets)\n",
    "\n",
    "    prefixed_cols = [(\"a_\" + col) for col in df.columns]\n",
    "    prefixed_partition_cols = [(\"a_\" + col) for col in partition_columns]\n",
    "    add_prefix_map = dict(zip(df.columns, prefixed_cols))\n",
    "    remove_prefix_map = dict(zip(prefixed_cols, df.columns))\n",
    "\n",
    "    df = rename_columns(df, **add_prefix_map)\n",
    "    show(df)\n",
    "\n",
    "    part_df = df.groupBy(*prefixed_partition_cols).count()\n",
    "    show(part_df)\n",
    "    part_df = part_df.toPandas()\n",
    "    part_df[\"num_buckets\"] = np.ceil(part_df[\"count\"] / float(max_rows_per_bucket)).astype(int)\n",
    "    part_df[\"beg\"] = part_df[\"num_buckets\"].cumsum() - part_df[\"num_buckets\"]\n",
    "    log(part_df, \"\\npartitions counts\")\n",
    "\n",
    "    partition_map = {}\n",
    "    for _, row in part_df.iterrows():\n",
    "        partition_map.update({tuple(row[prefixed_partition_cols]): (row.beg, row.num_buckets)})\n",
    "    log(partition_map, \"\\nrepartition data\")\n",
    "    partition_map_bc = df.sql_ctx.sparkSession.sparkContext.broadcast(partition_map)\n",
    "\n",
    "    @sqlfn.udf(returnType=IntegerType())\n",
    "    def _partition_index(*cols):\n",
    "        beg, count = partition_map_bc.value[tuple(cols)]\n",
    "        return int(beg + np.random.randint(count))\n",
    "\n",
    "    indexed_df = show(df.withColumn(\"index\", _partition_index(*prefixed_partition_cols)).cache())    \n",
    "    bucket_df = show(\n",
    "        indexed_df.repartitionByRange(\n",
    "            max(part_df[\"num_buckets\"].sum(), 1),\n",
    "            \"index\"\n",
    "        )\n",
    "    ).drop(\"index\")\n",
    "\n",
    "    return rename_columns(bucket_df, **remove_prefix_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_hive(df, database, table, max_rows_per_bucket, overwrite=True, raise_on_missing_columns=True):\n",
    "    spark = df.sql_ctx.sparkSession\n",
    "    columns = spark.catalog.listColumns(dbName=database, tableName=table)\n",
    "    log(columns, \"\\ncatalog columns\")\n",
    "\n",
    "    if not raise_on_missing_columns:\n",
    "        df = df.select(\n",
    "            *[\n",
    "                sqlfn.col(column.name) if column.name in df.columns else sqlfn.lit(None).alias(column.name)\n",
    "                for column in columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    main_columns, partition_columns = [], []\n",
    "\n",
    "    for column in columns:\n",
    "        if column.isPartition:\n",
    "            partition_columns.append(column.name)\n",
    "        else:\n",
    "            main_columns.append(column.name)\n",
    "\n",
    "    old_mode = spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    log(old_mode, \"\\nsaved spark.sql.sources.partitionOverwriteMode\")\n",
    "\n",
    "    try:\n",
    "        create_partition_buckets(\n",
    "            df.select(*(main_columns + partition_columns)), max_rows_per_bucket, *partition_columns\n",
    "        ).write.insertInto(\"{}.{}\".format(database, table), overwrite=overwrite)\n",
    "    finally:\n",
    "        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", old_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_hive(df, database=db, table=table, max_rows_per_bucket=1, raise_on_missing_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments with Spark SQL functions\n",
    "# https://spark.apache.org/docs/latest/api/sql/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter array values\n",
    "df_0 = show(spark.sql(\n",
    "    \"select filter(\"\n",
    "    \"array(\"\n",
    "    \"cast(1 as float), cast(null as float), cast('NaN' as float), cast(4 as float)\"\n",
    "    \"), _x -> not isnull(_x) and not isnan(_x)\"\n",
    "    \") as arrcol\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "df_1 = show(\n",
    "    spark.sql(\n",
    "        \"select map_from_arrays(\"\n",
    "        \"array('a', 'b', 'c', '101', '-1', '303'), \"\n",
    "        \"array(cast(1 as double), cast(null as double), cast('NaN' as double), 1.01, 2.02, 3.03)\"\n",
    "        \") as mapcol\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert map to array\n",
    "df_2 = show(df_1.selectExpr(\n",
    "    \"user_dmdesc.map_key_values(mapcol) as arrtuples\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop invalid tuples\n",
    "df_3 = show(df_2.selectExpr(\n",
    "    \"filter(arrtuples, _x -> \"\n",
    "    \"not isnull(_x['value']) and not isnan(_x['value']) and is_uint32(_x['key'])\"\n",
    "    \") as arrtuples\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert array to map\n",
    "df_4 = show(df_3.selectExpr(\n",
    "    \"cast(\"\n",
    "    \"map_from_entries(arrtuples)\"\n",
    "    \"as map<string,float>) as mapcol\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select cast(0 as float)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"test_features\")\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
