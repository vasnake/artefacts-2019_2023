[DEFAULT]
pool: dev.q.regular
queue: dev.q.regular
spark_conf: {
    "spark.yarn.maxAppAttempts": "1",
    "spark.driver.maxResultSize": "1G",
    "spark.executor.memoryOverhead": "2G",
    "spark.dynamicAllocation.enabled": "true",
    "spark.dynamicAllocation.maxExecutors": 15,
    "spark.dynamicAllocation.executorIdleTimeout": "300s",
    "spark.network.timeout": "1200s",
    "spark.locality.wait": "15s",
    "spark.reducer.maxReqsInFlight": "10",
    "spark.shuffle.io.retryWait": "60s",
    "spark.shuffle.io.maxRetries": "10",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryoserializer.buffer.max": "1024m",
    "spark.hadoop.mapred.output.compress": "true",
    "spark.hadoop.mapred.output.compression.codec": "org.apache.hadoop.io.compress.GzipCodec"
    }

[ExportAudienceTrgTask]
output_max_rows_per_file: 1000000
success_hdfs_basedir: hdfs:/data/dev/apps/export/audience/trg/
target_hdfs_basedir: hdfs:/data/dev/apps/export/audience/trg/data/

[spark]
master: yarn
deploy-mode: cluster
driver-cores: 2
driver-memory: 2G
num-executors: 10
executor-cores: 4
executor-memory: 4G
jars: hdfs:/lib/proto2-1.28.2.jar

[hive]
release: metastore

[core]
no_lock: true
logging_conf_file: prj/logs/config/luigi.cfg
error-email:

[retcode]
# Exit codes for Luigi.
# Some codes equal to zero because we don't want record failure when data is not ready
task_failed=30
scheduling_error=35
unhandled_exception=40
already_running=0
missing_data=0
not_run=0
