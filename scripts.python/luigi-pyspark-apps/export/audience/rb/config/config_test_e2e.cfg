[DEFAULT]
pool: dev.q.priority
queue: dev.q.priority
spark_conf: {
    "spark.yarn.maxAppAttempts": "1",
    "spark.driver.maxResultSize": "1G",
    "spark.executor.memoryOverhead": "1G",
    "spark.dynamicAllocation.enabled": "true",
    "spark.dynamicAllocation.maxExecutors": 2,
    "spark.dynamicAllocation.executorIdleTimeout": "300s",
    "spark.network.timeout": "1200s",
    "spark.locality.wait": "15s",
    "spark.reducer.maxReqsInFlight": "10",
    "spark.shuffle.io.retryWait": "60s",
    "spark.shuffle.io.maxRetries": "10",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryoserializer.buffer.max": "1024m",
    "spark.hadoop.mapred.output.compress": "false"
    }

[ExportAudienceRbTask]
output_max_rows_per_file: 1000000

[spark]
master: yarn
deploy-mode: cluster
driver-cores: 2
driver-memory: 2G
num-executors: 2
executor-cores: 2
executor-memory: 2G

[hive]
release: metastore

[core]
no_lock: true
logging_conf_file: dmgrinder/logs/config/luigi.cfg
error-email: dm-dev-grinder@we.mail.ru

[retcode]
# Exit codes for Luigi.
# Some codes equal to zero because we don't want record failure when data is not ready
task_failed=30
scheduling_error=35
unhandled_exception=40
already_running=0
missing_data=0
not_run=0
