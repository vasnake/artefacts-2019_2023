[DEFAULT]
pool: dev.q.regular
queue: dev.q.regular
spark_conf: {
    "spark.yarn.maxAppAttempts": "1",
    "spark.driver.maxResultSize": "1G",
    "spark.executor.memoryOverhead": "4G",
    "spark.dynamicAllocation.enabled": "true",
    "spark.dynamicAllocation.maxExecutors": 100,
    "spark.dynamicAllocation.executorIdleTimeout": "300s",
    "spark.network.timeout": "1200s",
    "spark.reducer.maxReqsInFlight": "10",
    "spark.shuffle.io.retryWait": "60s",
    "spark.shuffle.io.maxRetries": "10",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryoserializer.buffer.max": "1024m"
    }

[ExportUniversalFeaturesTask]
target_hdfs_basedir: hdfs:/data/dev/apps/export/universal_features/
csv_params: {
    "header": true,
    "delimiter": ";",
    "quote": "",
    "collection_items_sep": ",",
    "map_kv_sep": ":"
    }

[spark]
master: yarn
deploy-mode: cluster
driver-cores: 2
driver-memory: 2G
num-executors: 10
executor-cores: 4
executor-memory: 8G

[hive]
release: metastore

[core]
no_lock: true
logging_conf_file: prj/logs/config/luigi.cfg
error-email:

[retcode]
# Exit codes for Luigi.
# Some codes equal to zero because we don't want record failure when data is not ready
task_failed=30
scheduling_error=35
unhandled_exception=40
already_running=0
missing_data=0
not_run=0
