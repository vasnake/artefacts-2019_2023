[DEFAULT]
pool: dev.q.priority
queue: dev.q.priority
spark_conf: {
    "spark.yarn.maxAppAttempts": "1",
    "spark.driver.maxResultSize": "1G",
    "spark.executor.memoryOverhead": "1G",
    "spark.dynamicAllocation.enabled": "true",
    "spark.dynamicAllocation.maxExecutors": 2,
    "spark.dynamicAllocation.executorIdleTimeout": "300s",
    "spark.network.timeout": "1200s",
    "spark.reducer.maxReqsInFlight": "10",
    "spark.shuffle.io.retryWait": "60s",
    "spark.shuffle.io.maxRetries": "10",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryoserializer.buffer.max": "1024m",
    "spark.hadoop.hive.exec.dynamic.partition": "true",
    "spark.hadoop.hive.exec.dynamic.partition.mode" : "nonstrict",
    "spark.hadoop.hive.exec.max.dynamic.partitions": "1000",
    "spark.hadoop.hive.exec.max.dynamic.partitions.pernode": "1000"
    }

[CombineUniversalFeaturesTask]
output_max_rows_per_bucket: 101

[spark]
master: yarn
deploy-mode: cluster
driver-cores: 2
driver-memory: 2G
num-executors: 2
executor-cores: 2
executor-memory: 2G

[hive]
release: metastore

[core]
no_lock: true
logging_conf_file: prj/logs/config/luigi.cfg
error-email:

[retcode]
# Exit codes for Luigi.
# Some codes equal to zero because we don't want record failure when data is not ready
already_running=0
missing_data=0
not_run=0
task_failed=30
scheduling_error=35
unhandled_exception=40
