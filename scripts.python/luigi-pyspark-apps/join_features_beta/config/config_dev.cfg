[DEFAULT]
pool: dev.q.regular
queue: dev.q.regular
spark_conf: {
    "spark.yarn.maxAppAttempts": "1",
    "spark.driver.maxResultSize": "1G",
    "spark.executor.memoryOverhead": "4G",
    "spark.dynamicAllocation.enabled": "true",
    "spark.dynamicAllocation.maxExecutors": "150",
    "spark.dynamicAllocation.executorIdleTimeout": "300s",
    "spark.network.timeout": "1200s",
    "spark.reducer.maxReqsInFlight": "10",
    "spark.shuffle.io.retryWait": "60s",
    "spark.shuffle.io.maxRetries": "10",
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
    "spark.kryoserializer.buffer.max": "1024m",
    "spark.hadoop.hive.exec.dynamic.partition": "true",
    "spark.hadoop.hive.exec.dynamic.partition.mode": "nonstrict",
    "spark.hadoop.hive.exec.max.dynamic.partitions": "100000",
    "spark.hadoop.hive.exec.max.dynamic.partitions.pernode": "10000"
    }

[JoinerFeaturesTask]
output_max_rows_per_bucket: 1205505
shuffle_partitions: 8192
min_target_rows: 100000
checkpoint_interval: 15
executor_memory_gb: 16

[JoinerFeaturesSubTask]

[spark]
master: yarn
deploy-mode: cluster
driver-cores: 2
driver-memory: 4G
num-executors: 10
executor-cores: 4
executor-memory: 16G

[hive]
release: metastore

[core]
no_lock: true
logging_conf_file: prj/logs/config/luigi.cfg

[retcode]
# Exit codes for Luigi.
# Some codes equal to zero because we don't want record failure when data is not ready
already_running=0
missing_data=0
not_run=0
task_failed=30
scheduling_error=35
unhandled_exception=40
